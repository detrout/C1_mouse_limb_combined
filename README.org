* Introduction

Originally I was trying to collect input data for our C1 paper.

We had a final filter set of 433 cells. Since there was new data, 
I then committed my original code [[orgit-rev:/ssh:pongo:/woldlab/castor/home/diane/proj/C1_mouse_limb_combined/::0c9d57e][/ssh:pongo:proj/C1_mouse_limb_combined/ (magit-rev 0c9d57e)]]

* Create publication set at the DCC

[[file:find_experiment.sql::\pset%20format%20unaligned][find_experment.sql]] generates [[file:publication_files.tsv::experiment_accession%20experiment_description%20file%20filename%20file_type%20term_name%20age%20age_units%20library%20biosample%20library_created][publication_files.tsv]] which was 
used by [[file:create_publication_set.py][file:create_publication_set.py]] plus
associate all of the fastqs from our 433 set with the encode publication
https://www.encodeproject.org/publications/e0d01543-9965-4edb-933c-778a40575cd9/

* Bulk combine coverage?

I didn't write down this when I did it, so I'm not quite sure why I
did this.

I think there was a desire to compare single cell coverage vs bulk
experiments, and unfortunately our bulk experiments were done a long
time ago and we didn't have the coverage and distribution files.

Instead of re-running the whole thing, I downloaded the .bam files and
ran the necessary fragments of long-rna-seq-condor.

I downloaded with [[file:bulk/download.sh][bulk/download.sh]] and ran
[[file:bulk/calculate_coverage.dagman][bulk/calculate_coverage.dagman]] and
[[file:bulk/calculate_distribution.dagman][file:bulk/caculate_distribution.dagman]]

Since regenerating that is space intensive I'm going to cache the
result file. [[file:bulk_mouse_limb_combined_coverage.tsv][bulk_mouse_limb_combined_coverage.tsv]]

* Generate genes called plot for 100 specific cells

At some point Brian provided
[[file:list_of_100_single_cell_libraries_for_genes_called.xlsx][list_of_100_single_cell_libraries_for_genes_called.xlsx]]
and asked for the genes detected plots for those cells.
I used [[file:100_selected_genes_detected.py][100_selected_genes_detected.py]] to generate the merged csv file
[[file:list_of_100_single_cell_libraries.csv][list_of_100_single_cell_libraries.csv]].

I made the plot with my standard plot_genes_detected code, see below

#+BEGIN_SRC bash
export PYTHONPATH=~/proj/long-rna-seq-condor/
python3 -m woldrnaseq.plot_genes_detected  \
  --gtf-cache ~/proj/genome/mm10-M4-male/mm10-M4-male.h5 \
  list_of_100_single_cell_libraries.csv
#+END_SRC

* Request <2018-03-21 Wed>

Today I receved a request to compute our coverage "metric" for all of the new data.
The coverage metric was computing the slope of the middle of the coverage plot
[[file:/ssh:pongo:/woldlab/castor/home/diane/proj/long-rna-seq-condor/woldrnaseq/plot_coverage.py::def%20make_median_normalized_summary(experiments,%20coverage):][Coverage slope plot]]

I added the "new" list of cells to [[file:to_include.py::def%20generate_to_include_as_of_run17():][generate_to_include_as_of_run17]]

The information I received from Peng suggested I needed to compute the
slope for all the data, so we could find any outliers

And that suggests making a histogram of the slopes.

I just talked to Barbara and she wants the plots for each of the runs.

I unfortunately don't have a map between run & library.

** Implementation

First step was to generate combined FPKM and coverate quantification files.
(First attempt)
Run [[file:combine_coverage.py][combine_coverage.py]] to generate [[file:C1_mouse_limb_coverage_asof_run17.tsv][C1_mouse_limb_coverage_asof_run17.tsv]]

Compute the slope using [[file:compute_slope.py][compute_slope.py]]

* Request <2018-03-23 Fri>

Brian had a spread sheet [[file:Slope_match_PacBio_cells_Diane_March23_2018.xlsx][Slope_match_PacBio_cells_Diane_March23_2018.xlsx]] and he wanted
me to merge the two columns.

Which I did with [[file:slope_match.py][slope_match.py]].

However he also wanted me to generate the genes-detected values for
the C1 mouse limb data which took me a while, because I needed to ask
Sean which versions of the source genes-detected files to use.

* Request <2018-04-05 Thu> from gigio.
  :LOGBOOK:
  CLOCK: [2018-04-06 Fri 15:00]--[2018-04-06 Fri 16:15] =>  1:15
  CLOCK: [2018-04-05 Thu 15:00]--[2018-04-05 Thu 17:10] =>  2:10
  :END:

He'd like to try and tell which isoform of a few genes was being used in the C1 mouse limb data.

ENSMUSG00000020167
ENSMUSG00000032228
ENSMUSG00000063659
ENSMUSG00000030189

['ENSMUSG00000020167','ENSMUSG00000032228','ENSMUSG00000063659','ENSMUSG00000030189']

Wrote [[file:generate_combined_transcript_C1.py]] to generate his output file

* Request <2018-04-17 Tue> from Brian: make a trackhub.

Brian gave me a set in [[file:Limb_cells_for_track_hub_splice_isoforms_red_green_black_only_April17_2018.xlsx][Limb_cells_for_track_hub_splice_isoforms_red_green_black_only_April17_2018.xlsx]]
that he wants a trackhub made of.

http://woldlab.caltech.edu/~diane/limb_cells_for_track_hub_splice_isoforms/C1_mouse_limb.hub.txt

The trackhub is in:
[[file:/ssh:pongo:/woldlab/loxcyc/home/diane/public_html/limb_cells_for_track_hub_splice_isoforms/C1_mouse_limb.hub.txt][~diane/public_html/limb_cells_for_track_hub_splice_isoforms/C1_mouse_limb.hub.txt]]

It took me a bit to get the bam files linked some place publically accessible.

But its working now.

* Work on generating round two submission

This is now using the pooled fastqs that the DCC wanted.

I started with [[file:prepare_submission_20180430.py][prepare_submission_20180430.py]] to generate
merge_20180430_fastqs.condor which generated the pooled fastqs.

I still need to generate the file metadata for the submission

** <2018-05-07 Mon>

Got around to committing [[file:prepare_submission_20180430.py][prepare_submission_20180430.py]] and adding
[[file:pandas_submission.py][pandas_submission.py]] and the spreadsheets that drove it to git.

They're probably not a complete solution to the problem of submitting
library + well ids to the DCC, but I think I'm making progress.

One other issue we had with submission was that some of the libraries
were unusable on one of the flowcells because there was a barcode
collision. So I had to make some updates to htsworkflow
 [[orgit-rev:~/proj/htsworkflow/::ce1e0df][Use a different RDF term for bad lanes (magit-rev ce1e0df)]]
to make it easier to ignore bad flowcell lanes.

For what its worth I think spreadsheets are a probably easier solution
than the turtle import

* Request <2018-05-07 Mon> from Brian generate transcript coverage
[[file:compare_lists_submitted_cells_vs_cells_in_paper_May4_2018.ods][compare_lists_submitted_cells_vs_cells_in_paper_May4_2018.ods]] sheet cells_in_paper

#+NAME: plot_median_as_of_run17_normalized_coverage.py
#+BEGIN_SRC python :results output
import sys
import pandas
from pandasodf import ODFReader

from woldrnaseq import plot_coverage


def main():
    coverage = pandas.read_csv('C1_mouse_limb_coverage_asof_run17.tsv', sep='\t')
    coverage_names = [x.replace('_clean', '').replace('_mm10', '') for x in coverage.columns]
    coverage.columns = coverage_names
    book = ODFReader('compare_lists_submitted_cells_vs_cells_in_paper_May4_2018.ods')
    cells = book.parse('cells_in_paper')

    paper_cells = set(cells['all_cells_in_manuscript'])
    found_cells = set(coverage_names)
    print('available coverage table shape', coverage.shape)
    print('cells just in paper', len(paper_cells.difference(found_cells)))
    print('cells just in all known set', len(found_cells.difference(paper_cells)))

    #experiment = pandas.DataFrame({
    #    'experiment': ['cells_in_manuscript'],
    #    'replicates': [[]],
    #})
    #experiment = experiment.set_index('experiment')
    #experiment.loc['cells_in_manuscript']['replicates'] = list(cells['all_cells_in_manuscript'].values)
    #print(experiment.head())

    # Apparently this function is still using the dict based experiments
    experiment = {'cells_in_manuscript': cells['all_cells_in_manuscript'].values}

    plot_coverage.make_median_normalized_summary(experiment, coverage)

if __name__ == '__main__':
    main()
#+END_SRC

#+NAME: cells in manuscript median normalized coverage
[[./cells_in_manuscript.median-normalized.coverage.png]]

* Request <2018-05-07 Mon> from Brian compare what's been submitted to what should be submitted.
[[file:compare_lists_submitted_cells_vs_cells_in_paper_May4_2018.ods][compare_lists_submitted_cells_vs_cells_in_paper_May4_2018.ods]] sheet should_be_submitted


#+BEGIN_SRC python :results output
  import sys
  import pandas
  from pandasodf import ODFReader
  from htsworkflow.submission.encoded import ENCODED
  from requests.exceptions import  HTTPError

  def main():
      book = ODFReader('compare_lists_submitted_cells_vs_cells_in_paper_May4_2018.ods')
      print(book.sheet_names)
      paper = book.parse('cells_in_paper')
      submitted = book.parse('should_be_submitted')

      server = ENCODED('www.encodeproject.org')
    
      paper_cells = set(paper['all_cells_in_manuscript'])
      submitted_cells = set(submitted['submitted_library_ids'])
      for cell in sorted(paper_cells.difference(submitted_cells)):
          alias = 'barbara-wold:{}'.format(cell)
          try: 
             body = server.get_json(alias)
              state = ''
          except HTTPError as e:
              state = 'Not found'
          print('{},{}'.format(cell,state))

      #print('cells just in submitted set', ','.join(sorted(submitted_cells.difference(paper_cells))))

  if __name__ == '__main__':
      main()
#+END_SRC

#+RESULTS:
#+begin_example
['cells_in_paper', 'should_be_submitted']
17329_C4,Not found
17330_D12,Not found
17330_D9,Not found
17331_E8,Not found
17332_F1,Not found
17332_F3,Not found
17332_F6,Not found
17332_F7,Not found
17332_F8,Not found
17333_G2,Not found
17333_G6,Not found
17333_G9,Not found
17334_H1,Not found
17334_H3,Not found
17334_H7,Not found
18042_A12,Not found
18042_A4,Not found
18044_C11,Not found
18045_D11,Not found
18045_D2,Not found
18045_D6,Not found
18046_E9,Not found
18048_G10,Not found
18049_H5,Not found
18087_F9,Not found
18251_A2,Not found
18253_C5,Not found
18254_D11,Not found
18254_D5,Not found
18256_F5,Not found
18256_F8,Not found
18256_F9,Not found
18257_G5,Not found
18259_B8,Not found
18260_C11,Not found
18260_C2,Not found
18260_C6,Not found
18261_D3,Not found
18261_D5,Not found
18263_A12,Not found
18263_A4,Not found
18264_B3,Not found
18264_B4,Not found
18264_B8,Not found
18265_C10,Not found
18265_C4,Not found
18266_D1,Not found
18266_D6,Not found
18267_E10,Not found
18267_E3,Not found
18267_E4,Not found
18267_E6,Not found
18268_F1,Not found
18268_F12,Not found
18268_F3,Not found
18268_F7,Not found
18268_F8,Not found
18311_A4,Not found
18312_B4,Not found
18312_B5,Not found
18313_C1,Not found
18313_C10,Not found
18313_C11,Not found
18313_C12,Not found
18313_C5,Not found
18313_C7,Not found
18313_C8,Not found
18314_D2,Not found
18314_D3,Not found
18314_D4,Not found
18314_D7,Not found
18315_E12,Not found
18315_E2,Not found
18315_E4,Not found
18315_E6,Not found
18316_F1,Not found
18316_F10,Not found
18316_F11,Not found
18316_F5,Not found
18317_G3,Not found
18317_G4,Not found
18317_G7,Not found
18317_G8,Not found
18317_G9,Not found
#+end_example
