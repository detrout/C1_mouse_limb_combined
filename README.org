* Introduction

Originally I was trying to collect input data for our C1 paper.

We had a final filter set of 433 cells. Since there was new data, 
I then committed my original code [[orgit-rev:/ssh:pongo:/woldlab/castor/home/diane/proj/C1_mouse_limb_combined/::0c9d57e][/ssh:pongo:proj/C1_mouse_limb_combined/ (magit-rev 0c9d57e)]]

* Create publication set at the DCC

[[file:find_experiment.sql::\pset%20format%20unaligned][find_experment.sql]] generates [[file:publication_files.tsv::experiment_accession%20experiment_description%20file%20filename%20file_type%20term_name%20age%20age_units%20library%20biosample%20library_created][publication_files.tsv]] which was 
used by [[file:create_publication_set.py][file:create_publication_set.py]] plus
associate all of the fastqs from our 433 set with the encode publication
https://www.encodeproject.org/publications/e0d01543-9965-4edb-933c-778a40575cd9/

* Bulk combine coverage?

I didn't write down this when I did it, so I'm not quite sure why I
did this.

I think there was a desire to compare single cell coverage vs bulk
experiments, and unfortunately our bulk experiments were done a long
time ago and we didn't have the coverage and distribution files.

Instead of re-running the whole thing, I downloaded the .bam files and
ran the necessary fragments of long-rna-seq-condor.

I downloaded with [[file:bulk/download.sh][bulk/download.sh]] and ran
[[file:bulk/calculate_coverage.dagman][bulk/calculate_coverage.dagman]] and
[[file:bulk/calculate_distribution.dagman][file:bulk/caculate_distribution.dagman]]

Since regenerating that is space intensive I'm going to cache the
result file. [[file:bulk_mouse_limb_combined_coverage.tsv][bulk_mouse_limb_combined_coverage.tsv]]

* Generate genes called plot for 100 specific cells

At some point Brian provided
[[file:list_of_100_single_cell_libraries_for_genes_called.xlsx][list_of_100_single_cell_libraries_for_genes_called.xlsx]]
and asked for the genes detected plots for those cells.
I used [[file:100_selected_genes_detected.py][100_selected_genes_detected.py]] to generate the merged csv file
[[file:list_of_100_single_cell_libraries.csv][list_of_100_single_cell_libraries.csv]].

I made the plot with my standard plot_genes_detected code, see below

#+BEGIN_SRC bash
export PYTHONPATH=~/proj/long-rna-seq-condor/
python3 -m woldrnaseq.plot_genes_detected  \
  --gtf-cache ~/proj/genome/mm10-M4-male/mm10-M4-male.h5 \
  list_of_100_single_cell_libraries.csv
#+END_SRC

* Request <2018-03-21 Wed>

Today I receved a request to compute our coverage "metric" for all of the new data.
The coverage metric was computing the slope of the middle of the coverage plot
[[file:/ssh:pongo:/woldlab/castor/home/diane/proj/long-rna-seq-condor/woldrnaseq/plot_coverage.py::def%20make_median_normalized_summary(experiments,%20coverage):][Coverage slope plot]]

I added the "new" list of cells to [[file:to_include.py::def%20generate_to_include_as_of_run17():][generate_to_include_as_of_run17]]

The information I received from Peng suggested I needed to compute the
slope for all the data, so we could find any outliers

And that suggests making a histogram of the slopes.

I just talked to Barbara and she wants the plots for each of the runs.

I unfortunately don't have a map between run & library.

** Implementation

First step was to generate combined FPKM and coverate quantification files.
(First attempt)
Run [[file:combine_coverage.py][combine_coverage.py]] to generate [[file:C1_mouse_limb_coverage_asof_run17.tsv][C1_mouse_limb_coverage_asof_run17.tsv]]

Compute the slope using [[file:compute_slope.py][compute_slope.py]]

* Request <2018-03-23 Fri>

Brian had a spread sheet [[file:Slope_match_PacBio_cells_Diane_March23_2018.xlsx][Slope_match_PacBio_cells_Diane_March23_2018.xlsx]] and he wanted
me to merge the two columns.

Which I did with [[file:slope_match.py][slope_match.py]].

However he also wanted me to generate the genes-detected values for
the C1 mouse limb data which took me a while, because I needed to ask
Sean which versions of the source genes-detected files to use.

* Request <2018-04-05 Thu> from gigio.
  :LOGBOOK:
  CLOCK: [2018-04-06 Fri 15:00]--[2018-04-06 Fri 16:15] =>  1:15
  CLOCK: [2018-04-05 Thu 15:00]--[2018-04-05 Thu 17:10] =>  2:10
  :END:

He'd like to try and tell which isoform of a few genes was being used in the C1 mouse limb data.

ENSMUSG00000020167
ENSMUSG00000032228
ENSMUSG00000063659
ENSMUSG00000030189

['ENSMUSG00000020167','ENSMUSG00000032228','ENSMUSG00000063659','ENSMUSG00000030189']

Wrote [[file:generate_combined_transcript_C1.py]] to generate his output file

* Request <2018-04-17 Tue> from Brian: make a trackhub.

Brian gave me a set in [[file:Limb_cells_for_track_hub_splice_isoforms_red_green_black_only_April17_2018.xlsx][Limb_cells_for_track_hub_splice_isoforms_red_green_black_only_April17_2018.xlsx]]
that he wants a trackhub made of.

http://woldlab.caltech.edu/~diane/limb_cells_for_track_hub_splice_isoforms/C1_mouse_limb.hub.txt

The trackhub is in:
[[file:/ssh:pongo:/woldlab/loxcyc/home/diane/public_html/limb_cells_for_track_hub_splice_isoforms/C1_mouse_limb.hub.txt][~diane/public_html/limb_cells_for_track_hub_splice_isoforms/C1_mouse_limb.hub.txt]]

It took me a bit to get the bam files linked some place publically accessible.

But its working now.

* Work on generating round two submission

This is now using the pooled fastqs that the DCC wanted.

I started with [[file:prepare_submission_20180430.py][prepare_submission_20180430.py]] to generate
merge_20180430_fastqs.condor which generated the pooled fastqs.

I still need to generate the file metadata for the submission

Diane
