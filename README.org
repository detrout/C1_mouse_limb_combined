* Introduction

Originally I was trying to collect input data for our C1 paper.

We had a final filter set of 433 cells. Since there was new data,
I then committed my original code [[orgit-rev:/ssh:pongo:/woldlab/castor/home/diane/proj/C1_mouse_limb_combined/::0c9d57e][/ssh:pongo:proj/C1_mouse_limb_combined/ (magit-rev 0c9d57e)]]

* Create publication set at the DCC

[[file:find_experiment.sql::\pset%20format%20unaligned][find_experment.sql]] generates [[file:publication_files.tsv::experiment_accession%20experiment_description%20file%20filename%20file_type%20term_name%20age%20age_units%20library%20biosample%20library_created][publication_files.tsv]] which was
used by [[file:create_publication_set.py][file:create_publication_set.py]] plus
associate all of the fastqs from our 433 set with the encode publication
https://www.encodeproject.org/publications/e0d01543-9965-4edb-933c-778a40575cd9/

* Bulk combine coverage?

I didn't write down this when I did it, so I'm not quite sure why I
did this.

I think there was a desire to compare single cell coverage vs bulk
experiments, and unfortunately our bulk experiments were done a long
time ago and we didn't have the coverage and distribution files.

Instead of re-running the whole thing, I downloaded the .bam files and
ran the necessary fragments of long-rna-seq-condor.

I downloaded with [[file:bulk/download.sh][bulk/download.sh]] and ran
[[file:bulk/calculate_coverage.dagman][bulk/calculate_coverage.dagman]] and
[[file:bulk/calculate_distribution.dagman][file:bulk/caculate_distribution.dagman]]

Since regenerating that is space intensive I'm going to cache the
result file. [[file:bulk_mouse_limb_combined_coverage.tsv][bulk_mouse_limb_combined_coverage.tsv]]

* Generate genes called plot for 100 specific cells

At some point Brian provided
[[file:list_of_100_single_cell_libraries_for_genes_called.xlsx][list_of_100_single_cell_libraries_for_genes_called.xlsx]]
and asked for the genes detected plots for those cells.
I used [[file:100_selected_genes_detected.py][100_selected_genes_detected.py]] to generate the merged csv file
[[file:list_of_100_single_cell_libraries.csv][list_of_100_single_cell_libraries.csv]].

I made the plot with my standard plot_genes_detected code, see below

#+BEGIN_SRC bash
export PYTHONPATH=~/proj/long-rna-seq-condor/
python3 -m woldrnaseq.plot_genes_detected  \
  --gtf-cache ~/proj/genome/mm10-M4-male/mm10-M4-male.h5 \
  list_of_100_single_cell_libraries.csv
#+END_SRC

* Request <2018-03-21 Wed> Compute coverage slope plot for new cells

Today I receved a request to compute our coverage "metric" for all of the new data.
The coverage metric was computing the slope of the middle of the coverage plot
[[file:/ssh:pongo:/woldlab/castor/home/diane/proj/long-rna-seq-condor/woldrnaseq/plot_coverage.py::def%20make_median_normalized_summary(experiments,%20coverage):][Coverage slope plot]]

I added the "new" list of cells to [[file:to_include.py::def%20generate_to_include_as_of_run17():][generate_to_include_as_of_run17]]

The information I received from Peng suggested I needed to compute the
slope for all the data, so we could find any outliers

And that suggests making a histogram of the slopes.

I just talked to Barbara and she wants the plots for each of the runs.

I unfortunately don't have a map between run & library.

** Implementation

First step was to generate combined FPKM and coverate quantification files.
(First attempt)
Run [[file:combine_coverage.py][combine_coverage.py]] to generate [[file:C1_mouse_limb_coverage_asof_run17.tsv][C1_mouse_limb_coverage_asof_run17.tsv]]

Compute the slope using [[file:compute_slope.py][compute_slope.py]]

* Request <2018-03-23 Fri> merge spread sheet Slope_match_PacBio_cells_Diane_March23_2018.xlsx for Brian

Brian had a spread sheet [[file:Slope_match_PacBio_cells_Diane_March23_2018.xlsx][Slope_match_PacBio_cells_Diane_March23_2018.xlsx]] and he wanted
me to merge the two columns.

Which I did with [[file:slope_match.py][slope_match.py]].

However he also wanted me to generate the genes-detected values for
the C1 mouse limb data which took me a while, because I needed to ask
Sean which versions of the source genes-detected files to use.

* Request <2018-04-05 Thu> from gigio.
  :LOGBOOK:
  CLOCK: [2018-04-06 Fri 15:00]--[2018-04-06 Fri 16:15] =>  1:15
  CLOCK: [2018-04-05 Thu 15:00]--[2018-04-05 Thu 17:10] =>  2:10
  :END:

He'd like to try and tell which isoform of a few genes was being used in the C1 mouse limb data.

ENSMUSG00000020167
ENSMUSG00000032228
ENSMUSG00000063659
ENSMUSG00000030189

['ENSMUSG00000020167','ENSMUSG00000032228','ENSMUSG00000063659','ENSMUSG00000030189']

Wrote [[file:generate_combined_transcript_C1.py]] to generate his output file

* Request <2018-04-17 Tue> from Brian: make a trackhub.

Brian gave me a set in [[file:Limb_cells_for_track_hub_splice_isoforms_red_green_black_only_April17_2018.xlsx][Limb_cells_for_track_hub_splice_isoforms_red_green_black_only_April17_2018.xlsx]]
that he wants a trackhub made of.

http://woldlab.caltech.edu/~diane/limb_cells_for_track_hub_splice_isoforms/C1_mouse_limb.hub.txt

The trackhub is in:
[[file:/ssh:pongo:/woldlab/loxcyc/home/diane/public_html/limb_cells_for_track_hub_splice_isoforms/C1_mouse_limb.hub.txt][~diane/public_html/limb_cells_for_track_hub_splice_isoforms/C1_mouse_limb.hub.txt]]

It took me a bit to get the bam files linked some place publically accessible.

But its working now.

(Notes from the future:

I needed to modify generate trackhub for new data...
so this trackhub was generated using this input file:

    'Limb_cells_for_track_hub_splice_isoforms_red_green_black_only_April17_2018.csv'

and the new command would be

#+BEGIN_SRC bash
generate_trackhub.py --hub C1_mouse_limb \
                     --short-name  "C1 mouse limb" \
                     --long-name "C1 mouse limb April 17 2018"\
                     Limb_cells_for_track_hub_splice_isoforms_red_green_black_only_April17_2018.xlsx
#+END_SRC


* Work on generating round two submission

This is now using the pooled fastqs that the DCC wanted.

I started with [[file:prepare_submission_20180430.py][prepare_submission_20180430.py]] to generate
merge_20180430_fastqs.condor which generated the pooled fastqs.

I still need to generate the file metadata for the submission

** <2018-05-07 Mon>

Got around to committing [[file:prepare_submission_20180430.py][prepare_submission_20180430.py]] and adding
[[file:pandas_submission.py][pandas_submission.py]] and the spreadsheets that drove it to git.

They're probably not a complete solution to the problem of submitting
library + well ids to the DCC, but I think I'm making progress.

One other issue we had with submission was that some of the libraries
were unusable on one of the flowcells because there was a barcode
collision. So I had to make some updates to htsworkflow
 [[orgit-rev:~/proj/htsworkflow/::ce1e0df][Use a different RDF term for bad lanes (magit-rev ce1e0df)]]
to make it easier to ignore bad flowcell lanes.

For what its worth I think spreadsheets are a probably easier solution
than the turtle import

* Request <2018-05-07 Mon> from Brian generate transcript coverage
[[file:compare_lists_submitted_cells_vs_cells_in_paper_May4_2018.ods][compare_lists_submitted_cells_vs_cells_in_paper_May4_2018.ods]] sheet cells_in_paper

#+NAME: plot_median_as_of_run17_normalized_coverage.py
#+BEGIN_SRC python :results output
import sys
import pandas
from pandasodf import ODFReader

from woldrnaseq import plot_coverage


def main():
    coverage = pandas.read_csv('C1_mouse_limb_coverage_asof_run17.tsv', sep='\t')
    coverage_names = [x.replace('_clean', '').replace('_mm10', '') for x in coverage.columns]
    coverage.columns = coverage_names
    book = ODFReader('compare_lists_submitted_cells_vs_cells_in_paper_May4_2018.ods')
    cells = book.parse('cells_in_paper')

    paper_cells = set(cells['all_cells_in_manuscript'])
    found_cells = set(coverage_names)
    print('available coverage table shape', coverage.shape)
    print('cells just in paper', len(paper_cells.difference(found_cells)))
    print('cells just in all known set', len(found_cells.difference(paper_cells)))

    #experiment = pandas.DataFrame({
    #    'experiment': ['cells_in_manuscript'],
    #    'replicates': [[]],
    #})
    #experiment = experiment.set_index('experiment')
    #experiment.loc['cells_in_manuscript']['replicates'] = list(cells['all_cells_in_manuscript'].values)
    #print(experiment.head())

    # Apparently this function is still using the dict based experiments
    experiment = {'cells_in_manuscript': cells['all_cells_in_manuscript'].values}

    plot_coverage.make_median_normalized_summary(experiment, coverage)

if __name__ == '__main__':
    main()
#+END_SRC

#+NAME: cells in manuscript median normalized coverage
[[./cells_in_manuscript.median-normalized.coverage.png]]

* Request <2018-05-07 Mon> from Brian compare what's been submitted to what should be submitted.
[[file:compare_lists_submitted_cells_vs_cells_in_paper_May4_2018.ods][compare_lists_submitted_cells_vs_cells_in_paper_May4_2018.ods]] sheet should_be_submitted

Message-ID: <DM6PR03MB3515995D17F635F25D20E31FA0850@DM6PR03MB3515.namprd03.prod.outlook.com>

#+BEGIN_SRC python :results output
  import sys
  import pandas
  from pandasodf import ODFReader
  from htsworkflow.submission.encoded import ENCODED
  from requests.exceptions import  HTTPError

  def main():
      book = ODFReader('compare_lists_submitted_cells_vs_cells_in_paper_May4_2018.ods')
      print(book.sheet_names)
      paper = book.parse('cells_in_paper')
      submitted = book.parse('should_be_submitted')

      server = ENCODED('www.encodeproject.org')

      paper_cells = set(paper['all_cells_in_manuscript'])
      submitted_cells = set(submitted['submitted_library_ids'])
      for cell in sorted(paper_cells.difference(submitted_cells)):
          alias = 'barbara-wold:{}'.format(cell)
          try:
             body = server.get_json(alias)
              state = ''
          except HTTPError as e:
              state = 'Not found'
          print('{},{}'.format(cell,state))

      #print('cells just in submitted set', ','.join(sorted(submitted_cells.difference(paper_cells))))

  if __name__ == '__main__':
      main()
#+END_SRC

#+RESULTS:
#+begin_example
['cells_in_paper', 'should_be_submitted']
17329_C4,Not found
17330_D12,Not found
17330_D9,Not found
17331_E8,Not found
17332_F1,Not found
17332_F3,Not found
17332_F6,Not found
17332_F7,Not found
17332_F8,Not found
17333_G2,Not found
17333_G6,Not found
17333_G9,Not found
17334_H1,Not found
17334_H3,Not found
17334_H7,Not found
18042_A12,Not found
18042_A4,Not found
18044_C11,Not found
18045_D11,Not found
18045_D2,Not found
18045_D6,Not found
18046_E9,Not found
18048_G10,Not found
18049_H5,Not found
18087_F9,Not found
18251_A2,Not found
18253_C5,Not found
18254_D11,Not found
18254_D5,Not found
18256_F5,Not found
18256_F8,Not found
18256_F9,Not found
18257_G5,Not found
18259_B8,Not found
18260_C11,Not found
18260_C2,Not found
18260_C6,Not found
18261_D3,Not found
18261_D5,Not found
18263_A12,Not found
18263_A4,Not found
18264_B3,Not found
18264_B4,Not found
18264_B8,Not found
18265_C10,Not found
18265_C4,Not found
18266_D1,Not found
18266_D6,Not found
18267_E10,Not found
18267_E3,Not found
18267_E4,Not found
18267_E6,Not found
18268_F1,Not found
18268_F12,Not found
18268_F3,Not found
18268_F7,Not found
18268_F8,Not found
18311_A4,Not found
18312_B4,Not found
18312_B5,Not found
18313_C1,Not found
18313_C10,Not found
18313_C11,Not found
18313_C12,Not found
18313_C5,Not found
18313_C7,Not found
18313_C8,Not found
18314_D2,Not found
18314_D3,Not found
18314_D4,Not found
18314_D7,Not found
18315_E12,Not found
18315_E2,Not found
18315_E4,Not found
18315_E6,Not found
18316_F1,Not found
18316_F10,Not found
18316_F11,Not found
18316_F5,Not found
18317_G3,Not found
18317_G4,Not found
18317_G7,Not found
18317_G8,Not found
18317_G9,Not found
#+end_example

Those were the libraries that weren't submitted...

However, Idan would prefer the whole thing to be replaced with the
newer pooled model.

Working on preparing submission, started altering
prepare_submission_20180430.py to support reading different files.

#+NAME run prepare submission
#+BEGIN_SRC bash
PYTHONPATH=/woldlab/loxcyc/home/diane/proj/htsworkflow:/woldlab/loxcyc/home/diane/src/pandasodf python3 \
  prepare_submission_20180430.py \
     --name submission-20180619 \
     --sheet cells_in_paper \
     --header 0 \
     compare_lists_submitted_cells_vs_cells_in_paper_May4_2018.ods
#+END_SRC

Unfortunately the mirror of htsworkflow is broken on felcat.


** <2018-06-20 Wed> Notes

Fixed. The felcat_demo settings file refered to the old varaiable name.

The spreadsheet
compare_lists_submitted_cells_vs_cells_in_paper_May4_2018.ods contains
all the cells...

I had been assuming it was updates to the first set, and not the new
set. but it pretty clearly contains references to the libraries in the
second set.


| ENCSR255XZG | c1_e13.5_mouse_limb_run8  | resubmit |
| ENCSR530WGW | c1_e13.5_mouse_limb_run4  | resubmit |
| ENCSR985VMP | c1_e12.5_mouse_limb_run7  | resubmit |
| ENCSR463HWT | c1_e11.5_mouse_limb_run6  | resubmit |
| ENCSR619BUX | c1_e11.0_mouse_limb_run5  | resubmit |
| ENCSR991VTV | c1_e10.5_mouse_limb_run3  | resubmit |
| ENCSR159DIX | c1_e10.5_mouse_limb_run1  | resubmit |
| ENCSR242DQC | c1_e10.5_mouse_limb_run2  | resubmit |


|             | C1_mouse_e11.5_forelimb_run10 |   |
| ENCSR938RJZ | c1_e14.0_mouse_limb_run13     |   |
| ENCSR546KIB | c1_e14.5_mouse_limb_run17     |   |
| ENCSR430OIC | c1_e15.5_mouse_limb_run14     |   |

** <2018-06-21 Thu>

Tracking by hand is a pain, lets write a program!

wrote [[file:check_paper_cells.py]] which generated tranche.cvs

#+BEGIN_SRC bash
python3 check_paper_cells.py --header 0 compare_lists_submitted_cells_vs_cells_in_paper_May4_2018.ods
#+END_SRC

All of the missing cells are in tranche 1. (AKA from before I
submitted as a pool).

I unfortunately also discovered that some how two libraries were in
two experiments.

| 19912_G1 | 2 | c1_e10.5_mouse_limb_run15, c1_e11.5_mouse_limb_run10 | ENCSR134JVO, ENCSR220RKA |
| 19915_B1 | 2 | c1_e14.5_mouse_limb_run17, c1_e11.0_mouse_limb_run16 | ENCSR546KIB, ENCSR160DGP |

Turns out the DCC supports substring matching and 19912_G1 also
matches 19912_G10.

I updated check_paper_cells to search by the library ID returned when
retrieving the library object given the alias.

(Also I moved the lookup into a function so I could call it as a one
off)

With that change the conflict appears to have been resolved.

Reran and looked at tranche again. Yes it has been fixed.

(I really should commit stuff)


** <2018-06-22 Fri>

How, did I submit the first 433?

Its not in encode3-curation, it's not in encode4-curation. Google
drive has [[https://docs.google.com/spreadsheets/d/1cV9qOFij8K1pjzPFxm-bFR2KlI4I191pVM0nuzxT2jQ/edit#gid=310682961][dcc_experiment_files]] which only has a single sheet.

C1_mouse_limb_combined has tranche.csv (a map of stuff at the DCC),
publication_files.tsv, and README.org

It's not in ~/woldlab/ENCODE either. (Zgrep does look in .ods files)

ENCSR255XZG

Found it! [[file:///home/diane/woldlab/ENCODE/C1-encode3-limb-2017.ods][c1-encode3-limb-2017.ods]] Thanks tracker search!

** <2018-06-25 Mon> New metadata file

[[file:~/woldlab/ENCODE/C1-encode3-limb-tranche1-resubmit.ods][file:~/woldlab/ENCODE/C1-encode3-limb-tranche1-resubmit.ods]] updated to
look like the tranche 2 2018-04 submission.

* Request <2018-06-04 Mon> From Brian make a new trackhub

The source xlsx is
[[BED_files_tracks_request_single_cell_black_yellow_June3_2018.xlsx]]

I hacked generate_trackhub some to hard code fewer things

#+NAME generate 20180604 signal trackhub
#+BEGIN_SRC bash
generate_trackhub.py --hub "C1_s20180604" \
                     --short-name "C1_201806_sig"\
                     --long-name "C1 2018 june 4 signal"\
                     --signal BED_files_tracks_request_single_cell_black_yellow_June3_2018.xlsx
#+END_SRC

#+NAME generate 20180604 read trackhub
#+BEGIN_SRC bash
PYTHONPATH=~/proj/trackhub python3 generate_trackhub.py \
    --hub "C1_r20180604" \
    --short-name "C1_201806_read" \
    --long-name "C1 2018 june 4 reads" \
    --reads BED_files_tracks_request_single_cell_black_yellow_June3_2018.xlsx
#+END_SRC
* <2018-07-09 Mon> Work on producing bigwigs for a subset of cluster cells


** Table of cluster memberships
[[file:C1_peng_20180710_cluster_memberships.txt::{'x18317_G10_mm10'}%20{'DarkRed'%20}][C1 cluster memberships peng 20180710]]
What a terrible format

** Quick test case

python3 make_cluster_bigwigs.py -o merged.bw ~sau/flowcells/HFNYNBCX2/20049_F3/20049_F3-mm10-M4-male_genome.bam ~sau/flowcells/HFNYNBCX2/20048_E12/20048_E12-mm10-M4-male_genome.bam

** My program didn't normalize the bigwigs, and apparently they are normalized somehow.

Around chr12:55,862,947-55,864,116
bigwigs were autoscale max was around 17  and 29
while I was seeing more like 85 for my bigwig.
Also my bigwig didn't go to zero, it just continued the previous value
all the way to the next block

I did find utilities in the ucsc tree, that with a lot of file
maniuplation can make a plausible bigwig file

bigWigMerge <list of bigwigs> out.bg
grep chr out.bg > filtered.bg  # to remove spikes
bedSort filtered.bg sorted.bg  # to sort it the way they want
bedGraphToBigWig sorted.bg chrom.sizes result.bw.

Generate the cluster bigwig files
#+BEGIN_SRC bash
python3 find_bigwigs.py > merge_bw_all.sh
nohup bash ./merge_bw_all.sh &
#+END_SRC

* <2018-07-18 Wed> paper Violin plot

For the paper they wanted a violin plot broken out by cell type.

Peng provided a gene number vs cell type file
[[file:violin-paper/peng-example-violin.txt::gene_index,cell_type][Cell type membership]]

I generatated a bunch of plots using a jupyter notebook.

I foolishly had been editing the same cell for a while while trying to
adjust colors, but that got me in trouble with barbara

It worked better to move each new attempt into its own cell
[[file:violin-paper/peng-violin.ipynb][violin-paper/peng-violin.ipynb]]

* <2018-08-06 Mon> recompute C1 coverage statistics

Brian wants me to recompute the median coverage plots with what we've
learned about coverage.

The original versions were with "all genes" and we want single gene
models.

** TODO compute coverage for the 920 C1 cells in the manuscript [2/3]

We needed updated versions of this:

[[file:/ssh:pongo:/woldlab/loxcyc/home/diane/proj/C1_mouse_limb_combined/cells_in_manuscript.median-normalized.coverage.png][C1_mouse_limb_combined/cells_in_manuscript.median-normalized.coverage.png]]

*** DONE using just single gene models.
    CLOSED: [2018-08-08 Wed 11:13]

    - State "DONE"       from "TODO"       [2018-08-08 Wed 11:13]

Runs on wold-clst-4 took several hours last night? why?

The file sizes of the bigwigs were pretty similar. It did turn out
that wold-clst-4 ended up running out of swap.

It looks like the bigwig file were of similiar sizes.

I learned GNU time supports more program statics, its not installed by
default.

It looks like the coverage program uses about 3G of memory.

*** DONE using only protein coding, single model genes
    CLOSED: [2018-08-09 Thu 10:29]

    - State "DONE"       from "TODO"       [2018-08-09 Thu 10:29]
*** TODO using Georgi's refseq annotation set.
Georgis reference is also mm9!
/woldlab/castor/home/georgi/genomes/mm9/Mus_musculus.NCBIM37.67.filtered.gtf

Downloaded [[ftp://ftp.ncbi.nlm.nih.gov/refseq/M_musculus/alignments/GCF_000001635.24_knownrefseq_alignments.gff3][GCF_000001635.24_knownrefseq_alignments.gff3]]
Made working dir [[file:single-cell-single-model-min-1000-refseq]]

Adjusted [[file:single-cell-single-model-min-1000-refseq/recompute-coverage.condor]]
with paths

refSeq using different attributes than gencode. Because of course.


** TODO compute coverage for the 920 C1 cells broken out by their peng cell type cluster membership

Separate them out by color clusters, and then do coverage plots for
each of the clusters.

Delayed because the median summary code isn't per
experiment... woldrnaseq/plot_coverage needs help.

** TODO Compute fixed coverage for the limb bulk libraries
Then corrected coverage plot for all the limb bulks (12 libraries),

| directories | brians list      |
|       15019 | 15019 e11.5 limb |
|       15020 | 15020 e11.5 limb |
|       15084 | 15084 e14.5 limb |
|       15085 | 15085 e14.5 limb |
|       16111 | 16110 e13.5 limb |
|       16112 | 16111 e13.5 limb |
|       16134 | 16134 e15.5 limb |
|       16135 | 16135 e15.5 limb |
|       16930 | 16930 e12.5 limb |
|       16931 | 16931 e12.5 limb |
|       17298 | 17298 e10.5 limb |
|       17299 | 17299 e10.5 limb |

*** <2018-08-24 Fri> built plots

first run qc-coverage.condor arguments
#+BEGIN_EXAMPLE
$(GENE_COVERAGE) --gtf $(GTF) $(IN) --output $(OUT) --print-list
#+END_EXAMPLE

Second run
#+BEGIN_EXAMPLE
$(GENE_COVERAGE) --gtf $(GTF) $(IN) --output $(OUT) --gene-normalization max
#+END_EXAMPLE

This means that the .geneList files are raw un-normalized values.
The .coverage files are max normalized.

In http://woldlab.caltech.edu/~diane/C1_mouse_limb_combined/bulk/
For each experiment
ENCSR216NEG-e14.5-limb.median.coverage.png
ENCSR347SQR-e13.5-limb.median.coverage.png
ENCSR541XZK-e11.5-limb.median.coverage.png
ENCSR750YSX-e12.5-limb.median.coverage.png
ENCSR830IQV-e15.5-limb.median.coverage.png
ENCSR968QHO-e10.5-limb.median.coverage.png

for all the experiments
ENCSR968QHO-e10.5-limb.median-normalized.coverage.bare.png
ENCSR968QHO-e10.5-limb.median-normalized.coverage.png

Detail coverage plots
http://woldlab.caltech.edu/~diane/C1_mouse_limb_combined/bulk/gene_coverage_detail/


** TODO Compute fixed coverage for all of the C1 bulk libraries.

There's about 130 total C1 bulk libraries.

Include spearman calculations: so basically re run the QC steps.

** TODO Compute spike detection probabilities for the 920 cells
** TODO Compute spike detection probabilities for the 920 cells by peng cell type cluster membership
** TODO Coverage for top 1000 protein coding genes

Can we try a short experiment?  Pull up any single cell library that you have already made a transcript coverage plot for.
Then, take the FPKM values for the top 1000 most highly expressed genes in that library (you can use protein coding), regardless of
model complexity.
* <2018-09-27 Thu> Jason told me that there were mixed length reads in 3 fastqs

| ENCFF690WSL | C1_mouse_e13.5_limb_mesenchyme_mm10_run4.fastq.gz |
| ENCFF610BCY | C1_mouse_e11.5_limb_mesenchyme_mm10_run6.fastq.gz |
| ENCFF035IIS | C1_mouse_e11.0_limb_mesenchyme_mm10_run5.fastq.gz |

First I committed all of the previous work in the
C1_mouse_limb_combined directory.

Then I modified prepare_submission_20180430 to pass the per fastq url
metadata into the code to generate the fastq merging condor file.

(Amusingly that made the program simpler and cleaner)

#+BEGIN_SRC bash
python3 prepare_submission_20180430.py  \
  --first-tranche \
  --name submission-20180926 \
  --sheet cells_in_paper \
  --header 0 \
  compare_lists_submitted_cells_vs_cells_in_paper_May4_2018.ods

#+END_SRC

It looks like the sheet to describe what files to upload is in the ods
file, so I can just modify that after they're finished building.

** <2018-09-28 Fri> Double checking fastqs

- Do we have the same total length?

#+BEGIN_SRC bash
for f in \
  C1_mouse_e13.5_limb_mesenchyme_mm10_run4_101.fastq.gz \
  C1_mouse_e13.5_limb_mesenchyme_mm10_run4_51.fastq.gz \
  C1_mouse_e13.5_limb_mesenchyme_mm10_run4.fastq.gz \
  C1_mouse_e11.0_limb_mesenchyme_mm10_run5_101.fastq.gz \
  C1_mouse_e11.0_limb_mesenchyme_mm10_run5_51.fastq.gz \
  C1_mouse_e11.0_limb_mesenchyme_mm10_run5.fastq.gz \
  C1_mouse_e11.5_limb_mesenchyme_mm10_run6_101.fastq.gz \
  C1_mouse_e11.5_limb_mesenchyme_mm10_run6_51.fastq.gz \
  C1_mouse_e11.5_limb_mesenchyme_mm10_run6.fastq.gz; do \
  echo $f $(zcat  $f | wc -l); \
done
#+END_SRC

| C1_mouse_e13.5_limb_mesenchyme_mm10_run4_101.fastq.gz |  89366592 |
| C1_mouse_e13.5_limb_mesenchyme_mm10_run4_51.fastq.gz  | 304263328 |
|                                                       | 393629920 |
| C1_mouse_e13.5_limb_mesenchyme_mm10_run4.fastq.gz     | 393629920 |
| C1_mouse_e11.0_limb_mesenchyme_mm10_run5_101.fastq.gz |  96328608 |
| C1_mouse_e11.0_limb_mesenchyme_mm10_run5_51.fastq.gz  | 446122700 |
|                                                       | 542451308 |
| C1_mouse_e11.0_limb_mesenchyme_mm10_run5.fastq.gz     | 542451308 |
| C1_mouse_e11.5_limb_mesenchyme_mm10_run6_101.fastq.gz | 348093872 |
| C1_mouse_e11.5_limb_mesenchyme_mm10_run6_51.fastq.gz  | 537210244 |
|                                                       | 885304116 |
| C1_mouse_e11.5_limb_mesenchyme_mm10_run6.fastq.gz     | 885304116 |


** Did we submit what we're supposed to?

| C1_e10.5_mouse_limb_run1_June6_2016            |  87 |
| C1_e10.5_mouse_limb_run2_June20_2016           |  56 |
| C1_e10.5_mouse_limb_run3_Dec5_2016             |  32 |
| C1_mouse_e11.0_limb_mesenchyme_mm10_run5       |  58 |
| C1_mouse_e11.5_limb_mesenchyme_mm10_run6       |  80 |
| C1_mouse_e12.5_limb_mesenchyme_mm10_run7       |  76 |
| C1_mouse_e13.5_limb_mesenchyme_mm10_run4       |  71 |
| C1_mouse_e13.5_limb_mesenchyme_mm10_run8       |  64 |
| C1_mouse_e10.5_forelimb_run15_January13AM_2018 |  18 |
| C1_mouse_e11.0_forelimb_run16_January13PM_2018 |  14 |
| C1_mouse_e11.5_forelimb_run10_December11_2017  |  77 |
| C1_mouse_e12.0_forelimb_run11_December12_2017  |  77 |
| C1_mouse_e13.0_forelimb_run12_December13_2017  |  72 |
| C1_mouse_e14.0_forelimb_run13_December14_2017  |  53 |
| C1_mouse_e14.5_forelimb_run17_January16_2018   |  27 |
| C1_mouse_e15.5_forelimb_run14_December15_2017  |  58 |
|                                                | 920 |
#+TBLFM: @17$2=vsum(@1$2..@16$2)

Paper sheet says 920 sum of aliases in tsvs sumbs to 920.
whew

** Barbara wants all the fastqs to be the same length. 50.

Built a set of submission metadata with "everything" included

#+BEGIN_SRC python :results output
import pandas
from pprint import pprint
df = pandas.read_csv('submission-20180928-flowcell-details.tsv', sep='\t')
pprint(set(df[df['read_length'] == 101]['experiment']))

#+END_SRC

#+RESULTS:
: {'C1_e10.5_mouse_limb_run3_Dec5_2016',
:  'C1_mouse_e13.5_limb_mesenchyme_mm10_run4',
:  'C1_mouse_e11.0_limb_mesenchyme_mm10_run5',
:  'C1_mouse_e11.5_limb_mesenchyme_mm10_run6',
:  'C1_mouse_e14.0_forelimb_run13_December14_2017'}

Those are the ones with 100 bp

** fastq replacements

| old         | submitted_file_name                                    | action  | replacment  |
| ENCFF567DVD | C1_e10.5_mouse_limb_run3_Dec5_2016.fastq.gz            | replace | ENCFF949JJP |
| ENCFF690WSL | C1_mouse_e13.5_limb_mesenchyme_mm10_run4.fastq.gz      | replace | ENCFF738JJC |
| ENCFF035IIS | C1_mouse_e11.0_limb_mesenchyme_mm10_run5.fastq.gz      | replace | ENCFF710CRO |
| ENCFF610BCY | C1_mouse_e11.5_limb_mesenchyme_mm10_run6.fastq.gz      | replace | ENCFF653CRU |
| ENCFF615WYP | C1_mouse_e13.5_limb_mesenchyme_mm10_run4_101.fastq.gz  | delete  |             |
| ENCFF634YJN | C1_mouse_e13.5_limb_mesenchyme_mm10_run4_51.fastq.gz   | delete  |             |
| ENCFF821PYB | C1_mouse_e11.0_limb_mesenchyme_mm10_run5_101.fastq.gz  | delete  |             |
| ENCFF310FGP | C1_mouse_e11.0_limb_mesenchyme_mm10_run5_51.fastq.gz   | delete  |             |
| ENCFF164AIS | C1_mouse_e11.5_limb_mesenchyme_mm10_run6_101.fastq.gz  | delete  |             |
| ENCFF121IGH | C1_mouse_e11.5_limb_mesenchyme_mm10_run6_51.fastq.gz   | delete  |             |
| ENCFF924SYO | C1_mouse_e14.0_forelimb_run13_December14_2017.fastq.gz | replace | ENCFF749NGB |

** <2018-10-01 Mon> Uploaded the replacement files today

I had to tack the run13 information from
submission-201804-flowcell-details.tsv to the end of
submission-20180926-flowcell-details+run13.tsv to get the
flowcel_details field to be filled out correctly.

** <2018-10-01 Mon> Jason reported that the second set doesn't have an insert size.

[[https://www.encodeproject.org/search/?type=Experiment&accession=ENCSR134JVO&accession=ENCSR160DGP&accession=ENCSR220RKA&accession=ENCSR182WHH&accession=ENCSR430OIC&accession=ENCSR546KIB&accession=ENCSR716HQB][Second set of experiments]]

I copied over the code to find the size_range from
C1-encode3-limb-tranche1-resubmit.ipynb to
C1_mouse_limb_submission_201804.ipynb and updated a few things so the
sheet would run.

This is what it found.

ENCLB142PSQ "size_range": "555-617",
ENCLB137HIJ "size_range": "354-653",
ENCLB588VQM "size_range": "555-678",
ENCLB685FNO "size_range": "487-571",
ENCLB573UWE "size_range": "472-558",
ENCLB812YXV "size_range": "482-560",
ENCLB673LNA "size_range": "354-598",
ENCLB736JSQ "size_range": "471-527",

* <2018-10-01 Mon> Jason found 5 experiments all labeled e13.5


| Experiment  | Description    | Date               |
| ENCSR311IKT | C1 e13.5 run 8 | September 26, 2018 |
| ENCSR839DYB | C1 e13.5 run 4 | September 28, 2018 |
| ENCSR787QXE | 10x            | January 29, 2018   |
| ENCSR530WGW | C1 e13.5 run 4 | July 1, 2017       |
| ENCSR255XZG | C1 e13.5 run 8 | June 30, 2017      |
|             |                |                    |

One of those is a 10x submission

two of those are the original many replicate submission, and two are
the new pooled submission.

The question is what, if any experiments should be merged together?

#+BEGIN_SRC python
  accessions = [
      'ENCSR723FBU', 'ENCSR062KGY', 'ENCSR559CDN', 'ENCSR881ZYX',
      'ENCSR182LFI', 'ENCSR652JLT', 'ENCSR839DYB', 'ENCSR311IKT',
      'ENCSR134JVO', 'ENCSR160DGP', 'ENCSR220RKA', 'ENCSR716HQB',
      'ENCSR182WHH', 'ENCSR938RJZ', 'ENCSR546KIB', 'ENCSR430OIC',
  ]

  from htsworkflow.submission.encoded import ENCODED

  server = ENCODED('www.encodeproject.org')
  server.load_netrc()

  results = []
  for accession in accessions:
      experiment = server.get_json(accession)
      for replicate in experiment['replicates']:
          library = replicate['library']
          biosample = library['biosample']
          results.append([
              accession,
              experiment['description'],
              library['accession'],
              biosample['accession'],
              biosample['date_obtained']])

  results = sorted(results, key=lambda x: x[1])
  results.insert(0, ['Experiment', 'Description', 'Library', 'Biosample', 'Date obtained'])
  return results
#+END_SRC

#+RESULTS:
| Experiment  | Description                    | Library     | Biosample   | Date obtained |
| ENCSR062KGY | C1 e10.5 mouse forelimb run 1  | ENCLB724RNK | ENCBS554MMC |    2016-06-06 |
| ENCSR723FBU | C1 e10.5 mouse forelimb run 2  | ENCLB971JGJ | ENCBS296NSC |    2016-06-20 |
| ENCSR559CDN | C1 e10.5 mouse forelimb run 3  | ENCLB864BFK | ENCBS439GGN |    2016-12-05 |
| ENCSR134JVO | C1 e10.5 mouse forelimb run 15 | ENCLB142PSQ | ENCBS183GJD |    2018-01-13 |
| ENCSR881ZYX | C1 e11.0 mouse forelimb run 5  | ENCLB442WTN | ENCBS661XMW |    2017-01-23 |
| ENCSR160DGP | C1 e11.0 mouse forelimb run 16 | ENCLB137HIJ | ENCBS790VEC |    2018-01-13 |
| ENCSR182LFI | C1 e11.5 mouse forelimb run 6  | ENCLB391AEB | ENCBS366ZCP |    2017-01-31 |
| ENCSR220RKA | C1 e11.5 mouse forelimb run 10 | ENCLB588VQM | ENCBS589YXS |    2017-12-11 |
| ENCSR716HQB | C1 e12.0 mouse forelimb run 11 | ENCLB685FNO | ENCBS875OEX |    2017-12-12 |
| ENCSR652JLT | C1 e12.5 mouse forelimb run 7  | ENCLB880HUG | ENCBS471HOV |    2017-02-01 |
| ENCSR182WHH | C1 e13.0 mouse forelimb run 12 | ENCLB573UWE | ENCBS800RRA |    2017-12-13 |
| ENCSR839DYB | C1 e13.5 mouse forelimb run 4  | ENCLB119UWL | ENCBS841DUB |    2017-01-19 |
| ENCSR311IKT | C1 e13.5 mouse forelimb run 8  | ENCLB948WTO | ENCBS306QOG |    2017-02-23 |
| ENCSR938RJZ | C1 e14.0 mouse forelimb run 13 | ENCLB812YXV | ENCBS621IQO |    2017-12-14 |
| ENCSR546KIB | C1 e14.5 mouse forelimb run 17 | ENCLB673LNA | ENCBS375BFM |    2018-01-16 |
| ENCSR430OIC | C1 e15.5 mouse forelimb run 14 | ENCLB736JSQ | ENCBS849XJR |    2017-12-15 |

After discussion it was decided it was best to leave them
separate. The dissections and library building work were done at
different times.

* <2018-10-01 Mon> Do we still have all the dagman files?

#+BEGIN_SRC python :results output
  import os
  from glob import glob
  from generate_combined_transcript_C1 import ASOF_RUN17_library_files

  commands = ['star_dir', 'rsem_dir']
  version_name = 'Version: '
  results = {}
  for name in ASOF_RUN17_library_files.split():
      name = name.strip()
      dirname, _ = os.path.split(os.path.expanduser(name))
      for dagman in glob(dirname + '/*.dagman'):
          with open(dagman, 'rt') as instream:
              for line in instream:
                  version = line.find(version_name)
                  if version >= 0:
                      version_id = line[version:].strip()
                      results.setdefault(dagman, {}).setdefault(version_name, set()).add(version_id)
                  for command in commands:
                      location = line.find(command)
                      if location >= 0:
                          path = line[location+len(command)+2: -2]
                          results.setdefault(dagman, {}).setdefault(command, set()).add(path)

          print(dagman)
          for command in commands:
              print('  ', results[dagman].get(command, ''))
          if version_name in results[dagman]:
              print('  ', results[dagman][version_name])

  #return results
#+END_SRC

#+RESULTS:
#+begin_example
/woldlab/castor/home/sau/flowcells/H5LV3BCXY/run.dagman
   {'/woldlab/castor/proj/programs/STAR-2.5.2a/source/'}
   {'/woldlab/castor/home/diane/proj/long-rna-seq-pipeline/rsem/'}
   {'Version: 0.9-69-gf738e7'}
/woldlab/castor/home/sau/flowcells/H5LV3BCXY/rebuild-bam.dagman
   {'/woldlab/castor/proj/programs/STAR-2.5.2a/source/'}

   {'Version: 0.9-69-gf738e7'}
/woldlab/castor/home/sau/flowcells/H5LV3BCXY/H5LV3BCXY.dagman
   {'/woldlab/castor/home/diane/proj/long-rna-seq-pipeline/star/source/'}
   {'/woldlab/castor/home/diane/proj/long-rna-seq-pipeline/rsem/'}
/woldlab/castor/home/sau/flowcells/C1_e10.5_mouse_limb_run2_June20_2016/single-cell.dagman
   {'/woldlab/castor/home/diane/proj/long-rna-seq-pipeline/star/source/'}
   {'/woldlab/castor/home/diane/proj/long-rna-seq-pipeline/rsem/'}
/woldlab/castor/home/sau/flowcells/C1_e10.5_mouse_limb_run2_June20_2016/rebuild-bam.dagman
   {'/woldlab/castor/proj/programs/STAR-2.5.2a/source/'}

   {'Version: 0.9-69-gf738e7'}
/woldlab/castor/home/sau/flowcells/C1_e10.5_mouse_limb_run2_June20_2016/run.dagman
   {'/woldlab/castor/proj/programs/STAR-2.5.2a/source/'}
   {'/woldlab/castor/home/diane/proj/long-rna-seq-pipeline/rsem/'}
   {'Version: 0.9-69-gf738e7'}
/woldlab/loxcyc/home/diane/proj/C1_e10.5_mouse_limb_run3_Dec5_2016_2/run.dagman
   {'/woldlab/castor/proj/programs/STAR-2.5.2a/bin/Linux_x86_64/'}
   {'/woldlab/castor/home/diane/proj/long-rna-seq-pipeline/rsem/'}
/woldlab/castor/home/sau/flowcells/C1_mouse_limb_combined_Mar_2017/run_mm10.dagman
   {'/woldlab/castor/proj/programs/STAR-2.5.2a/source/'}
   {'/woldlab/castor/home/diane/proj/long-rna-seq-pipeline/rsem/'}
/woldlab/castor/home/sau/flowcells/C1_mouse_limb_combined_Mar_2017/rebuild_bam.dagman
   {'/woldlab/castor/proj/programs/STAR-2.5.2a/source/'}

   {'Version: 0.9-69-gf738e7'}
/woldlab/castor/home/sau/flowcells/C1_mouse_limb_combined_Mar_2017/run.dagman
   {'/woldlab/castor/proj/programs/STAR-2.5.2a/source/'}
   {'/woldlab/castor/home/diane/proj/long-rna-seq-pipeline/rsem/'}
/woldlab/castor/home/sau/flowcells/C1_mouse_limb_combined_Mar_2017/run_mm10_clean.dagman
   {'/woldlab/castor/proj/programs/STAR-2.5.2a/source/'}
   {'/woldlab/castor/home/diane/proj/long-rna-seq-pipeline/rsem/'}
/woldlab/castor/home/sau/flowcells/C1_mouse_limb_combined_Mar_2017/new_seed.dagman
   {'/woldlab/castor/proj/programs/STAR-2.5.2a/source/'}
   {'/woldlab/castor/home/diane/proj/long-rna-seq-pipeline/rsem/'}
/woldlab/castor/home/sau/flowcells/HFNLNBCX2/run_trim.dagman
   {'/woldlab/castor/proj/programs/STAR-2.5.2a/source/'}
   {'/woldlab/castor/home/diane/proj/long-rna-seq-pipeline/rsem/'}
   {'Version: 0.9-69-gf738e7'}
/woldlab/castor/home/sau/flowcells/HFNLNBCX2/run-20090_C12.dagman
   {'/woldlab/castor/proj/programs/STAR-2.5.2a/source/'}
   {'/woldlab/castor/home/diane/proj/long-rna-seq-pipeline/rsem/'}
   {'Version: 0.9-69-gf738e7'}
/woldlab/castor/home/sau/flowcells/H7CNTBCX2/run.dagman
   {'/woldlab/castor/proj/programs/STAR-2.5.2a/source/'}
   {'/woldlab/castor/home/diane/proj/long-rna-seq-pipeline/rsem/'}
   {'Version: 0.9-32-g4adccc'}
/woldlab/castor/home/sau/flowcells/HFNLTBCX2/run.dagman
   {'/woldlab/castor/proj/programs/STAR-2.5.2a/source/'}
   {'/woldlab/castor/home/diane/proj/long-rna-seq-pipeline/rsem/'}
   {'Version: 0.9-69-gf738e7'}
/woldlab/castor/home/sau/flowcells/HF7NTBCX2/run.dagman
   {'/woldlab/castor/proj/programs/STAR-2.5.2a/source/'}
   {'/woldlab/castor/home/diane/proj/long-rna-seq-pipeline/rsem/'}
   {'Version: 0.9-69-gf738e7'}
/woldlab/castor/home/sau/flowcells/HFNYNBCX2/run.dagman
   {'/woldlab/castor/proj/programs/STAR-2.5.2a/source/'}
   {'/woldlab/castor/home/diane/proj/long-rna-seq-pipeline/rsem/'}
   {'Version: 0.9-69-gf738e7'}
#+end_example

We have all of them.... though... then there's the problem telling
them apart.

Argh face palm. RELEASE_VERSION file wasn't being updated. Also that
commit id doesn't seem to exist anymore.


* <2018-10-02 Tue> So what happens if we feed one of these merged fastqs to star

Create directory merged_run, create library.tsv and experiment.tsv


* <2018-10-02 Tue> Make a PublicationSet  (or find my previous one)

to make a collection of our submitted fastqs
* <2018-10-03 Wed> QC release

#+TBLNAME: c1_to_release
| https://encodeproject.org/experiments/ENCSR062KGY/ | C1 e10.5 mouse forelimb run 1  |
| https://encodeproject.org/experiments/ENCSR723FBU/ | C1 e10.5 mouse forelimb run 2  |
| https://encodeproject.org/experiments/ENCSR559CDN/ | C1 e10.5 mouse forelimb run 3  |
| https://encodeproject.org/experiments/ENCSR160DGP/ | C1 e11.0 mouse forelimb run 16 |
| https://encodeproject.org/experiments/ENCSR881ZYX/ | C1 e11.0 mouse forelimb run 5  |
| https://encodeproject.org/experiments/ENCSR220RKA/ | C1 e11.5 mouse forelimb run 10 |
| https://encodeproject.org/experiments/ENCSR182LFI/ | C1 e11.5 mouse forelimb run 6  |
| https://encodeproject.org/experiments/ENCSR716HQB/ | C1 e12.0 mouse forelimb run 11 |
| https://encodeproject.org/experiments/ENCSR652JLT/ | C1 e12.5 mouse forelimb run 7  |
| https://encodeproject.org/experiments/ENCSR182WHH/ | C1 e13.0 mouse forelimb run 12 |
| https://encodeproject.org/experiments/ENCSR839DYB/ | C1 e13.5 mouse forelimb run 4  |
| https://encodeproject.org/experiments/ENCSR311IKT/ | C1 e13.5 mouse forelimb run 8  |
| https://encodeproject.org/experiments/ENCSR938RJZ/ | C1 e14.0 mouse forelimb run 13 |
| https://encodeproject.org/experiments/ENCSR546KIB/ | C1 e14.5 mouse forelimb run 17 |
| https://encodeproject.org/experiments/ENCSR430OIC/ | C1 e15.5 mouse forelimb run 14 |
* <2018-10-08 Mon> Rerun run1 and 2

Since it looked like those were run with star 2.4.0k and not 2.5.2a that everything else was
we should rerun them and make sure everything looks reasonable.

I symlinked the fastqs from sean, into

[[file:H5LV3BCXY/]]
[[file:C1_e10.5_mouse_limb_run2_June20_2016]]

for a in ~sau/flowcells/H5LV3BCXY/*.fastq.gz; do ln -s $a ; done

for a in ~sau/flowcells/C1_e10.5_mouse_limb_run2_June20_2016/*.fastq.gz ; do ln -s $a ; done

tail -n +2 library.tsv | cut -f 2 | xargs mkdir

#+BEGIN_SRC
tail -n +2 library.tsv | cut -f 2 | xargs mkdir
PYTHONPATH=~sau/flowcells/rna_pipelines/long-rna-seq-condor/ python3 -m woldrnaseq.make_dag -l library.tsv > run.dagman
condor_submit_dag run.dagman
#+END_SRC

#+BEGIN_SRC bash
pushd H5LV3BCXY
PYTHONPATH=~sau/flowcells/rna_pipelines/long-rna-seq-condor/ \
  python3 -m woldrnaseq.makersemcsv -l library.tsv -e experiments.tsv
popd
pushd C1_e10.5_mouse_limb_run2_June20_2016
PYTHONPATH=~sau/flowcells/rna_pipelines/long-rna-seq-condor/ \
  python3 -m woldrnaseq.makersemcsv -l library.tsv -e experiments.tsv
popd
#+END_SRC

* <2018-10-09 Tue> Need to create a publication set for the bulk samples used in the paper

see [[file:create_bulk_publication_set.py]]

| ENCSR574CRQ | RNA-seq mouse forelimb of whole tissue embryonic developmental time course. |
| ENCSR226XLF | Single Cell RNA-seq mouse forelimb embryonic developmental time course.     |
|             |                                                                             |

Hmm... But what about just attaching datasets to the publication
object?

Resolved with: [[file:find_publication_dataset_ids.py::from%20htsworkflow.submission.encoded%20import%20ENCODED][find_publication_dataset_ids.py]]


#+RESULTS:
: None

* <2018-10-09 Tue> I probably should match dagmans up to what quantifications went into the master table

I looked in [[file:generate_combined_C1.py::~sau/flowcells/H5LV3BCXY/C1_e10.5_mouse_limb_run1_June6_2016_FPKM.csv][generate_combined_C1.py]] for the list of FPKM files used
and we definitely used the run1 and run2 files that were generated
with STAR 2.4.0k

So... how different _are_ they?

[[file:compare_star_versions.py][compare_star_versions.py]]

So... how different is the same version of star?

#+BEGIN_SRC bash
for d in H5LV3BCXY_2 H5LV3BCXY_3; do
  pushd $d
  for f in ~sau/flowcells/H5LV3BCXY/*.fastq.gz; do ln -s $f ; done
  cp ~sau/flowcells/H5LV3BCXY/library.tsv .
  cp ~sau/flowcells/H5LV3BCXY/experiments.tsv .
  tail -n +2 library.tsv | cut -f 2 | xargs mkdir
  PYTHONPATH=~sau/flowcells/rna_pipelines/long-rna-seq-condor/ \
    python3 -m woldrnaseq.make_dag -l library.tsv > run.dagman
  popd
done
#+END_SRC

#+RESULTS:
| ~/proj/C1_mouse_limb_combined/H5LV3BCXY_2 | ~/proj/C1_mouse_limb_combined |
| ~/proj/C1_mouse_limb_combined             |                               |
| ~/proj/C1_mouse_limb_combined/H5LV3BCXY_3 | ~/proj/C1_mouse_limb_combined |
| ~/proj/C1_mouse_limb_combined             |                               |

#+BEGIN_SRC bash
for d in H5LV3BCXY H5LV3BCXY_2 H5LV3BCXY_3; do
  pushd $d >> /dev/null
  for q in FPKM TPM expected_count; do
    PYTHONPATH=~sau/flowcells/rna_pipelines/long-rna-seq-condor/ \
      python3 -m woldrnaseq.makersemcsv -l library.tsv -e experiments.tsv \
              -q ${q}
  done
  popd >> /dev/null
done
#+END_SRC

#+RESULTS:

#+BEGIN_SRC bash
for q in FPKM TPM expected_count; do
  for d in H5LV3BCXY H5LV3BCXY_2 H5LV3BCXY_3; do
  pushd $d >> /dev/null
    sha256sum C1_e10.5_mouse_limb_run1_June6_2016_gene_${q}.csv
  popd >> /dev/null
  done
done
#+END_SRC

#+RESULTS:
| b64b9b263e10ebc6c15f616869d9793753b06d3461e61fe1aa32cdd9c6e5a470 | C1_e10.5_mouse_limb_run1_June6_2016_gene_FPKM.csv           |
| b64b9b263e10ebc6c15f616869d9793753b06d3461e61fe1aa32cdd9c6e5a470 | C1_e10.5_mouse_limb_run1_June6_2016_gene_FPKM.csv           |
| b64b9b263e10ebc6c15f616869d9793753b06d3461e61fe1aa32cdd9c6e5a470 | C1_e10.5_mouse_limb_run1_June6_2016_gene_FPKM.csv           |
| 8ea439e69727d441de54b6a2eec7dd316bedc821d2ff78459408ad2c4172e936 | C1_e10.5_mouse_limb_run1_June6_2016_gene_TPM.csv            |
| 8ea439e69727d441de54b6a2eec7dd316bedc821d2ff78459408ad2c4172e936 | C1_e10.5_mouse_limb_run1_June6_2016_gene_TPM.csv            |
| 8ea439e69727d441de54b6a2eec7dd316bedc821d2ff78459408ad2c4172e936 | C1_e10.5_mouse_limb_run1_June6_2016_gene_TPM.csv            |
| f069df455ff7c9b38eba5e86ec3d1ab71d145e4fdad512ae4cd38b8a7fa593fe | C1_e10.5_mouse_limb_run1_June6_2016_gene_expected_count.csv |
| 9ea2d2758cea6f668c06e3b15938ceb808512e413e80dfd1e338f7b8498f51ac | C1_e10.5_mouse_limb_run1_June6_2016_gene_expected_count.csv |
| 0b57e6111e7177fe2fdb22d471f09f695ec0f87b6e7e06bd6f5d977bdc159c09 | C1_e10.5_mouse_limb_run1_June6_2016_gene_expected_count.csv |
                                 C1_e10.5_mouse_limb_run1_June6_2016_gene_FPKM
Make sure run2 reproduces
#+BEGIN_SRC bash
for d in C1_e10.5_mouse_limb_run2_June20_2016_2 C1_e10.5_mouse_limb_run2_June20_2016_3; do
  mkdir $d
  pushd $d >> /dev/null
  for f in ~sau/flowcells/C1_e10.5_mouse_limb_run2_June20_2016/*.fastq.gz; do ln -s $f ; done
  cp ~sau/flowcells/C1_e10.5_mouse_limb_run2_June20_2016/library.tsv .
  cp ~sau/flowcells/C1_e10.5_mouse_limb_run2_June20_2016/experiments.tsv .
  tail -n +2 library.tsv | cut -f 2 | xargs mkdir
  PYTHONPATH=~sau/flowcells/rna_pipelines/long-rna-seq-condor/ \
    python3 -m woldrnaseq.make_dag -l library.tsv > run.dagman
  popd >> /dev/null
done
#+END_SRC

#+BEGIN_SRC bash
for d in C1_e10.5_mouse_limb_run2_June20_2016 \
         C1_e10.5_mouse_limb_run2_June20_2016_2 \
         C1_e10.5_mouse_limb_run2_June20_2016_3; do
  pushd $d >> /dev/null
  for q in FPKM TPM expected_count; do
    PYTHONPATH=~sau/flowcells/rna_pipelines/long-rna-seq-condor/ \
      python3 -m woldrnaseq.makersemcsv -l library.tsv -e all-in-one-experiment.tsv \
              -q $q
  done
  popd >> /dev/null
done
#+END_SRC

#+RESULTS:


#+BEGIN_SRC bash
for q in FPKM TPM expected_count; do
  for d in C1_e10.5_mouse_limb_run2_June20_2016 \
           C1_e10.5_mouse_limb_run2_June20_2016_2 \
           C1_e10.5_mouse_limb_run2_June20_2016_3; do
  pushd $d >> /dev/null
    sha256sum C1_e10.5_mouse_limb_run2_June20_2016_gene_${q}.csv
  popd >> /dev/null
  done
done
#+END_SRC

#+RESULTS:
| 166027605587ee167db523eb7a483b1f40f42f35573c38537f4976e98f94d001 | C1_e10.5_mouse_limb_run2_June20_2016_gene_FPKM.csv           |
| 166027605587ee167db523eb7a483b1f40f42f35573c38537f4976e98f94d001 | C1_e10.5_mouse_limb_run2_June20_2016_gene_FPKM.csv           |
| 166027605587ee167db523eb7a483b1f40f42f35573c38537f4976e98f94d001 | C1_e10.5_mouse_limb_run2_June20_2016_gene_FPKM.csv           |
| 3bc1ba838e682edb1c37d16900c4a1f73b6451757490be07fa4d0a10938cbdf9 | C1_e10.5_mouse_limb_run2_June20_2016_gene_TPM.csv            |
| 3bc1ba838e682edb1c37d16900c4a1f73b6451757490be07fa4d0a10938cbdf9 | C1_e10.5_mouse_limb_run2_June20_2016_gene_TPM.csv            |
| 3bc1ba838e682edb1c37d16900c4a1f73b6451757490be07fa4d0a10938cbdf9 | C1_e10.5_mouse_limb_run2_June20_2016_gene_TPM.csv            |
| 7ecf2cb62bfe421260b6a9a1263855ff44c7797fee098cc17a47eeddef4d9cf5 | C1_e10.5_mouse_limb_run2_June20_2016_gene_expected_count.csv |
| 15b223b1f262dd5cbd81ef15c9d08657e38e0e14b2d56acd4cbd65cef1ef6612 | C1_e10.5_mouse_limb_run2_June20_2016_gene_expected_count.csv |
| 5f76acd9f0537898d496316adb755de95cc6f4ed6012a650377d7f454fd31d51 | C1_e10.5_mouse_limb_run2_June20_2016_gene_expected_count.csv |

Henry wondered if the ReadsPerGene.out.tab was reproducible
Run 1
#+BEGIN_SRC bash
for d in H5LV3BCXY H5LV3BCXY_2 H5LV3BCXY_3; do
  pushd $d >> /dev/null
  for a in $(find . -name ReadsPerGene.out.tab | sort); do sha256sum $a ; done | sha256sum
  popd >> /dev/null
done
#+END_SRC

#+RESULTS:
| 684042af8299c2b2c0748db95f1d839adb9e7b780ad114959ed29f51989d0930 | 0 |
| 684042af8299c2b2c0748db95f1d839adb9e7b780ad114959ed29f51989d0930 | 0 |
| 684042af8299c2b2c0748db95f1d839adb9e7b780ad114959ed29f51989d0930 | 0 |

Run 2
#+BEGIN_SRC bash
for d in C1_e10.5_mouse_limb_run2_June20_2016 \
         C1_e10.5_mouse_limb_run2_June20_2016_2 \
         C1_e10.5_mouse_limb_run2_June20_2016_3; do
  pushd $d >> /dev/null
  for a in $(find . -name ReadsPerGene.out.tab | sort); do sha256sum $a ; done | sha256sum
  popd >> /dev/null
done
#+END_SRC

#+RESULTS:
| 244e3574879f1ff59ef5584d680f1c12dbaa8eec24e383a04735265a582c4924 | 0 |
| 244e3574879f1ff59ef5584d680f1c12dbaa8eec24e383a04735265a582c4924 | 0 |
| 244e3574879f1ff59ef5584d680f1c12dbaa8eec24e383a04735265a582c4924 | 0 |


#+BEGIN_SRC python :results output
  import pandas
  import os

  df = pandas.read_csv('compare_bams_genome.csv', index_col=0)

  for column in df:
      data = df[column]
      if len(set(data)) > 1:
          print(column, set(data))

  print('done')

#+END_SRC

#+RESULTS:
: done

Implies that all of the genome alignments are reproducible

* Are all the expected bulk libraries present?
  <2018-10-15 Mon>

#+BEGIN_SRC python
  import pandas
  from htsworkflow.submission.encoded import ENCODED

  server = ENCODED('www.encodeproject.org')

  sheet = pandas.read_excel(
      'Mouse embryo samples list library numbers Diane August 21 2017.xlsx',
      dtype={'Library number': str,
             'biorep': str})

  status = []
  for library_id in sheet['Library number']:
      if not (pandas.isnull(library_id) or library_id == 'nan'):
          url = '/library/barbara-wold:{}/'.format(library_id)
          library = server.get_json(url)
          status.append((library['accession'], library['status']))

  return status

#+END_SRC

#+RESULTS:
| ENCLB029BTN | released |
| ENCLB286HEN | released |
| ENCLB257LEX | released |
| ENCLB396XZV | released |
| ENCLB554ZAM | released |
| ENCLB838FEJ | released |
| ENCLB293YOF | released |
| ENCLB247JXW | released |
| ENCLB564JOZ | released |
| ENCLB688RET | released |
| ENCLB537WPK | released |
| ENCLB682KXD | released |
| ENCLB835LVO | released |
| ENCLB471QMM | released |
| ENCLB659UFY | released |
| ENCLB426JKF | released |
| ENCLB454ZUS | released |
| ENCLB061TDP | released |
| ENCLB331VAA | released |
| ENCLB761PCA | released |
| ENCLB601XLL | released |
| ENCLB347FRI | released |
| ENCLB055KYV | released |
| ENCLB357KYA | released |
| ENCLB495FUZ | released |
| ENCLB979DZW | released |
| ENCLB162EJJ | released |
| ENCLB627LDZ | released |
| ENCLB204IPP | released |
| ENCLB214HEI | released |
| ENCLB839VOV | released |
| ENCLB187TEZ | released |
| ENCLB408EFI | released |
| ENCLB008IOE | released |
| ENCLB613DIH | released |
| ENCLB230FGL | released |
| ENCLB656OMB | released |
| ENCLB637CNF | released |
| ENCLB968JLC | released |
| ENCLB432GJF | released |
| ENCLB878ALV | released |
| ENCLB673NRF | released |
| ENCLB222KWY | released |
| ENCLB224ETE | released |
| ENCLB177SFU | released |
| ENCLB807ODU | released |
| ENCLB051RZO | released |
| ENCLB426MFX | released |
| ENCLB312IAF | released |
| ENCLB517UIN | released |
| ENCLB032PUJ | released |
| ENCLB304NDN | released |
| ENCLB690TFQ | released |
| ENCLB351WRP | released |
| ENCLB238AFW | released |
| ENCLB724HXC | released |
| ENCLB504REE | released |
| ENCLB863LLT | released |
| ENCLB302GKB | released |
| ENCLB327XFF | released |
| ENCLB336GOL | released |
| ENCLB964APA | released |
| ENCLB000EUQ | released |
| ENCLB597GRF | released |
| ENCLB989QNS | released |
| ENCLB567WFL | released |
| ENCLB254AMJ | released |
| ENCLB015WCJ | released |
| ENCLB200EFP | released |
| ENCLB278MMD | released |
| ENCLB819PZM | released |
| ENCLB220DVX | released |
| ENCLB906TKN | released |
| ENCLB949YQN | released |
| ENCLB584VFZ | released |
| ENCLB931ORG | released |
| ENCLB409LKR | released |
| ENCLB595PNG | released |
| ENCLB156EMG | released |
| ENCLB030NEC | released |
| ENCLB377PCX | released |
| ENCLB214UZM | released |
| ENCLB813BQB | released |
| ENCLB949WIO | released |
| ENCLB373ANW | released |
| ENCLB567JQE | released |
| ENCLB073YDX | released |
| ENCLB028XWK | released |
| ENCLB507CWD | released |
| ENCLB614EHD | released |
| ENCLB526KEI | released |
| ENCLB835IBO | released |
| ENCLB873UUE | released |
| ENCLB975KAO | released |
| ENCLB215FCS | released |
| ENCLB599UVX | released |
| ENCLB079XWZ | released |
| ENCLB042OTC | released |
| ENCLB753FMG | released |
| ENCLB110FBI | released |
| ENCLB783FQC | released |
| ENCLB209CGK | released |
| ENCLB646LEJ | released |
| ENCLB105VLP | released |
| ENCLB150ZJS | released |
| ENCLB256STD | released |
| ENCLB082UHO | released |
| ENCLB672ISU | released |
| ENCLB430WXG | released |
| ENCLB026UUS | released |
| ENCLB733PDT | released |
| ENCLB841PIC | released |
| ENCLB662TIG | released |
| ENCLB987VRV | released |
| ENCLB696ITY | released |
| ENCLB535BBV | released |
| ENCLB489BJY | released |
| ENCLB424YVG | released |
| ENCLB123VIC | released |
| ENCLB626WMA | released |
| ENCLB287BOO | released |
| ENCLB008KBU | released |
| ENCLB496KJW | released |
| ENCLB540BPI | released |
| ENCLB852NKM | released |
| ENCLB732ZDB | released |
| ENCLB266LCY | released |
| ENCLB055JUC | released |
| ENCLB080NNG | released |
| ENCLB180OTB | released |
| ENCLB274VUA | released |
| ENCLB441AFS | released |
| ENCLB790ZKD | released |
| ENCLB319NLX | released |
| ENCLB074REG | released |
| ENCLB415KPR | released |
| ENCLB658ICO | released |
| ENCLB741KQB | released |
| ENCLB810TRL | released |
| ENCLB544VIE | released |
| ENCLB370ZFK | released |
| ENCLB273BPC | released |
| ENCLB847UDV | released |
| ENCLB704CYQ | released |
| ENCLB318WHF | released |
| ENCLB590UZK | released |
| ENCLB260QNG | released |
| ENCLB817LXB | released |
| ENCLB169SNA | released |
| ENCLB459OYG | released |
| ENCLB217DSV | released |
| ENCLB159SLV | released |
| ENCLB022VFG | released |
| ENCLB416HZP | released |
| ENCLB556YSG | released |
| ENCLB803HJK | released |

* Bulk version I previous downloaded was aligned against a newer reference set
  <2018-10-18 Thu>

so it doesn't compare with my current version.

So lets download the fastqs and take it from the top.

#+BEGIN_SRC python :results output
  from htsworkflow.submission.encoded import ENCODED
  import os

  server = ENCODED('www.encodeproject.org')
  experiments = ['ENCSR541XZK','ENCSR216NEG','ENCSR347SQR','ENCSR830IVQ','ENCSR750YSX','ENCSR968QHO',]
  with open('bulk/download_fastq.sh', 'wt') as outstream:
      for accession in experiments:
          experiment = server.get_json('/experiments/{}/'.format(accession))
          for f in experiment['files']:
              if f['file_type'] == 'fastq':
                  pathname = f.get('submitted_file_name')
                  library_id = pathname[:5]
                  filename = os.path.split(pathname)[1]
                  target = os.path.join(library_id, filename)
                  href = 'https://www.encodeproject.org' + f.get('href')
                  #print(accession, f['accession'], library_id, filename)
                  outstream.write(('curl -L -o {} {}'.format(target, href)))
                  outstream.write(os.linesep)
#+END_SRC

* Cranioface DCC reproducable.
  <2018-10-24 Wed>

some issues reproducing replicates...
is it other tissues too?
heart lung kidney stomach hindbrain also impacted.

doing a technical replication on heart.
there may be "sno"? left

e14.5 / P0

* Barbara wants "the new transcript coverage algorithm" run on mouse bulk libraries
  <2018-10-31 Wed>

Barbara has asked if you can run the following libraries through your
new transcript coverage algorithm, using the new low end prevalence
filters:

15019
15020
16110
16111

I started figuring out how to do this over in the
encode3-rna-evaluation project [[file:/scp:galvar:/woldlab/loxcyc/home/diane/proj/encode3-rna-evaluation/README.org::*<2018-10-30%20Tue>%20threshold%20looks%20promising][<2018-10-30 Tue> threshold looks promising]]

#+NAME: bulk_refseq_coverage
bulk/refseq-GRCm38.p6

#+BEGIN_SRC bash :var coverage_dir=bulk_refseq_coverage :results none
if [ \! -d ${coverage_dir} ]; then
    mkdir -p ${coverage_dir}
fi
pushd ${coverage_dir}
cp ../library.tsv .
cp ../experiments.tsv .
for d in $(tail -n +2 library.tsv | cut -f 2); do
  mkdir $d
done
popd
#+END_SRC

#+BEGIN_SRC bash :var coverage_dir=bulk_refseq_coverage :results none
pushd ${coverage_dir}
for a in $(tail -n +2 library.tsv | cut -f 2); do
  pushd $a
  ln -s ../../$a/Signal.Unique.str1.out.bg
  popd
done
popd
#+END_SRC

#+BEGIN_SRC bash :var coverage_dir=bulk_refseq_coverage :results none
pushd ${coverage_dir}
for d in $(tail -n +2 library.tsv | cut -f 2); do
  pushd $d ;
  (python3 ~/proj/GeorgiScripts/gene_coverage_wig_gtf.py \
    --gtf ~/proj/genome/GRCm38.p6+spikes.gtf \
    Signal.Unique.str1.out.bg --output $d-mm10-M4-male.coverage \
    --print-list | tee coverage.out) &
   popd ;
done
popd
#+END_SRC

#+BEGIN_SRC bash :var coverage_dir=bulk_refseq_coverage :results none
pushd ${coverage_dir}
python3 -m woldrnaseq.plot_coverage -l library.tsv -e experiments.tsv
popd
#+END_SRC

Well that didn't work.

The chromosome names in the GRCm38 file look like NC_000067.6, which
is clearly not similar to chr1. So the quantification is zero, because
nothing is found.

* Plot for Barbara's Keystone presentation C1 data by clusters in illumina
** Setup

Goal is to have jupyter notebook local while kernel is running on galvar

#+BEGIN_SRC emacs-lisp :results none
    (let* ((local_jupyter "/run/user/1000/jupyter/")
           (remote_jupyter "/scp:galvar.caltech.edu:/run/user/6850/jupyter/")
           (json "kernel-a36ad8eb-5b6e-45b4-82ef-8c0c734e3db4.json")
           (local (concat local_jupyter json))
           (remote (concat remote_jupyter json)))
      (progn
        (copy-file remote local 't)
        (start-process "jupyter" "*jupyter-shell*"
                       "jupyter" "notebook" "--existing" json "--ssh" "galvar")
        )
     )
#+END_SRC
