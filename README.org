* Introduction

Originally I was trying to collect input data for our C1 paper.

We had a final filter set of 433 cells. Since there was new data,
I then committed my original code [[orgit-rev:/ssh:pongo:/woldlab/castor/home/diane/proj/C1_mouse_limb_combined/::0c9d57e][/ssh:pongo:proj/C1_mouse_limb_combined/ (magit-rev 0c9d57e)]]

* Create publication set at the DCC

[[file:find_experiment.sql::\pset%20format%20unaligned][find_experment.sql]] generates [[file:publication_files.tsv::experiment_accession%20experiment_description%20file%20filename%20file_type%20term_name%20age%20age_units%20library%20biosample%20library_created][publication_files.tsv]] which was
used by [[file:create_publication_set.py][file:create_publication_set.py]] plus
associate all of the fastqs from our 433 set with the encode publication
https://www.encodeproject.org/publications/e0d01543-9965-4edb-933c-778a40575cd9/

* Bulk combine coverage?

I didn't write down this when I did it, so I'm not quite sure why I
did this.

I think there was a desire to compare single cell coverage vs bulk
experiments, and unfortunately our bulk experiments were done a long
time ago and we didn't have the coverage and distribution files.

Instead of re-running the whole thing, I downloaded the .bam files and
ran the necessary fragments of long-rna-seq-condor.

I downloaded with [[file:bulk/download.sh][bulk/download.sh]] and ran
[[file:bulk/calculate_coverage.dagman][bulk/calculate_coverage.dagman]] and
[[file:bulk/calculate_distribution.dagman][file:bulk/caculate_distribution.dagman]]

Since regenerating that is space intensive I'm going to cache the
result file. [[file:bulk_mouse_limb_combined_coverage.tsv][bulk_mouse_limb_combined_coverage.tsv]]

* Generate genes called plot for 100 specific cells

At some point Brian provided
[[file:list_of_100_single_cell_libraries_for_genes_called.xlsx][list_of_100_single_cell_libraries_for_genes_called.xlsx]]
and asked for the genes detected plots for those cells.
I used [[file:100_selected_genes_detected.py][100_selected_genes_detected.py]] to generate the merged csv file
[[file:list_of_100_single_cell_libraries.csv][list_of_100_single_cell_libraries.csv]].

I made the plot with my standard plot_genes_detected code, see below

#+BEGIN_SRC bash
export PYTHONPATH=~/proj/long-rna-seq-condor/
python3 -m woldrnaseq.plot_genes_detected  \
  --gtf-cache ~/proj/genome/mm10-M4-male/mm10-M4-male.h5 \
  list_of_100_single_cell_libraries.csv
#+END_SRC

* Request <2018-03-21 Wed> Compute coverage slope plot for new cells

Today I receved a request to compute our coverage "metric" for all of the new data.
The coverage metric was computing the slope of the middle of the coverage plot
[[file:/ssh:pongo:/woldlab/castor/home/diane/proj/long-rna-seq-condor/woldrnaseq/plot_coverage.py::def%20make_median_normalized_summary(experiments,%20coverage):][Coverage slope plot]]

I added the "new" list of cells to [[file:to_include.py::def%20generate_to_include_as_of_run17():][generate_to_include_as_of_run17]]

The information I received from Peng suggested I needed to compute the
slope for all the data, so we could find any outliers

And that suggests making a histogram of the slopes.

I just talked to Barbara and she wants the plots for each of the runs.

I unfortunately don't have a map between run & library.

** Implementation

First step was to generate combined FPKM and coverate quantification files.
(First attempt)
Run [[file:combine_coverage.py][combine_coverage.py]] to generate [[file:C1_mouse_limb_coverage_asof_run17.tsv][C1_mouse_limb_coverage_asof_run17.tsv]]

Compute the slope using [[file:compute_slope.py][compute_slope.py]]

* Request <2018-03-23 Fri> merge spread sheet Slope_match_PacBio_cells_Diane_March23_2018.xlsx for Brian

Brian had a spread sheet [[file:Slope_match_PacBio_cells_Diane_March23_2018.xlsx][Slope_match_PacBio_cells_Diane_March23_2018.xlsx]] and he wanted
me to merge the two columns.

Which I did with [[file:slope_match.py][slope_match.py]].

However he also wanted me to generate the genes-detected values for
the C1 mouse limb data which took me a while, because I needed to ask
Sean which versions of the source genes-detected files to use.

* Request <2018-04-05 Thu> from gigio.
  :LOGBOOK:
  CLOCK: [2018-04-06 Fri 15:00]--[2018-04-06 Fri 16:15] =>  1:15
  CLOCK: [2018-04-05 Thu 15:00]--[2018-04-05 Thu 17:10] =>  2:10
  :END:

He'd like to try and tell which isoform of a few genes was being used in the C1 mouse limb data.

ENSMUSG00000020167
ENSMUSG00000032228
ENSMUSG00000063659
ENSMUSG00000030189

['ENSMUSG00000020167','ENSMUSG00000032228','ENSMUSG00000063659','ENSMUSG00000030189']

Wrote [[file:generate_combined_transcript_C1.py]] to generate his output file

* Request <2018-04-17 Tue> from Brian: make a trackhub.

Brian gave me a set in [[file:Limb_cells_for_track_hub_splice_isoforms_red_green_black_only_April17_2018.xlsx][Limb_cells_for_track_hub_splice_isoforms_red_green_black_only_April17_2018.xlsx]]
that he wants a trackhub made of.

http://woldlab.caltech.edu/~diane/limb_cells_for_track_hub_splice_isoforms/C1_mouse_limb.hub.txt

The trackhub is in:
[[file:/ssh:pongo:/woldlab/loxcyc/home/diane/public_html/limb_cells_for_track_hub_splice_isoforms/C1_mouse_limb.hub.txt][~diane/public_html/limb_cells_for_track_hub_splice_isoforms/C1_mouse_limb.hub.txt]]

It took me a bit to get the bam files linked some place publically accessible.

But its working now.

(Notes from the future:

I needed to modify generate trackhub for new data...
so this trackhub was generated using this input file:

    'Limb_cells_for_track_hub_splice_isoforms_red_green_black_only_April17_2018.csv'

and the new command would be

#+BEGIN_SRC bash
generate_trackhub.py --hub C1_mouse_limb \
                     --short-name  "C1 mouse limb" \
                     --long-name "C1 mouse limb April 17 2018"\
                     Limb_cells_for_track_hub_splice_isoforms_red_green_black_only_April17_2018.xlsx
#+END_SRC


* Work on generating round two submission

This is now using the pooled fastqs that the DCC wanted.

I started with [[file:prepare_submission_20180430.py][prepare_submission_20180430.py]] to generate
merge_20180430_fastqs.condor which generated the pooled fastqs.

I still need to generate the file metadata for the submission

** <2018-05-07 Mon>

Got around to committing [[file:prepare_submission_20180430.py][prepare_submission_20180430.py]] and adding
[[file:pandas_submission.py][pandas_submission.py]] and the spreadsheets that drove it to git.

They're probably not a complete solution to the problem of submitting
library + well ids to the DCC, but I think I'm making progress.

One other issue we had with submission was that some of the libraries
were unusable on one of the flowcells because there was a barcode
collision. So I had to make some updates to htsworkflow
 [[orgit-rev:~/proj/htsworkflow/::ce1e0df][Use a different RDF term for bad lanes (magit-rev ce1e0df)]]
to make it easier to ignore bad flowcell lanes.

For what its worth I think spreadsheets are a probably easier solution
than the turtle import

* Request <2018-05-07 Mon> from Brian generate transcript coverage
[[file:compare_lists_submitted_cells_vs_cells_in_paper_May4_2018.ods][compare_lists_submitted_cells_vs_cells_in_paper_May4_2018.ods]] sheet cells_in_paper

#+NAME: plot_median_as_of_run17_normalized_coverage.py
#+BEGIN_SRC python :results output
import sys
import pandas
from pandasodf import ODFReader

from woldrnaseq import plot_coverage


def main():
    coverage = pandas.read_csv('C1_mouse_limb_coverage_asof_run17.tsv', sep='\t')
    coverage_names = [x.replace('_clean', '').replace('_mm10', '') for x in coverage.columns]
    coverage.columns = coverage_names
    book = ODFReader('compare_lists_submitted_cells_vs_cells_in_paper_May4_2018.ods')
    cells = book.parse('cells_in_paper')

    paper_cells = set(cells['all_cells_in_manuscript'])
    found_cells = set(coverage_names)
    print('available coverage table shape', coverage.shape)
    print('cells just in paper', len(paper_cells.difference(found_cells)))
    print('cells just in all known set', len(found_cells.difference(paper_cells)))

    #experiment = pandas.DataFrame({
    #    'experiment': ['cells_in_manuscript'],
    #    'replicates': [[]],
    #})
    #experiment = experiment.set_index('experiment')
    #experiment.loc['cells_in_manuscript']['replicates'] = list(cells['all_cells_in_manuscript'].values)
    #print(experiment.head())

    # Apparently this function is still using the dict based experiments
    experiment = {'cells_in_manuscript': cells['all_cells_in_manuscript'].values}

    plot_coverage.make_median_normalized_summary(experiment, coverage)

if __name__ == '__main__':
    main()
#+END_SRC

#+NAME: cells in manuscript median normalized coverage
[[./cells_in_manuscript.median-normalized.coverage.png]]

* Request <2018-05-07 Mon> from Brian compare what's been submitted to what should be submitted.
[[file:compare_lists_submitted_cells_vs_cells_in_paper_May4_2018.ods][compare_lists_submitted_cells_vs_cells_in_paper_May4_2018.ods]] sheet should_be_submitted

Message-ID: <DM6PR03MB3515995D17F635F25D20E31FA0850@DM6PR03MB3515.namprd03.prod.outlook.com>

#+BEGIN_SRC python :results output
  import sys
  import pandas
  from pandasodf import ODFReader
  from htsworkflow.submission.encoded import ENCODED
  from requests.exceptions import  HTTPError

  def main():
      book = ODFReader('compare_lists_submitted_cells_vs_cells_in_paper_May4_2018.ods')
      print(book.sheet_names)
      paper = book.parse('cells_in_paper')
      submitted = book.parse('should_be_submitted')

      server = ENCODED('www.encodeproject.org')

      paper_cells = set(paper['all_cells_in_manuscript'])
      submitted_cells = set(submitted['submitted_library_ids'])
      for cell in sorted(paper_cells.difference(submitted_cells)):
          alias = 'barbara-wold:{}'.format(cell)
          try:
             body = server.get_json(alias)
              state = ''
          except HTTPError as e:
              state = 'Not found'
          print('{},{}'.format(cell,state))

      #print('cells just in submitted set', ','.join(sorted(submitted_cells.difference(paper_cells))))

  if __name__ == '__main__':
      main()
#+END_SRC

#+RESULTS:
#+begin_example
['cells_in_paper', 'should_be_submitted']
17329_C4,Not found
17330_D12,Not found
17330_D9,Not found
17331_E8,Not found
17332_F1,Not found
17332_F3,Not found
17332_F6,Not found
17332_F7,Not found
17332_F8,Not found
17333_G2,Not found
17333_G6,Not found
17333_G9,Not found
17334_H1,Not found
17334_H3,Not found
17334_H7,Not found
18042_A12,Not found
18042_A4,Not found
18044_C11,Not found
18045_D11,Not found
18045_D2,Not found
18045_D6,Not found
18046_E9,Not found
18048_G10,Not found
18049_H5,Not found
18087_F9,Not found
18251_A2,Not found
18253_C5,Not found
18254_D11,Not found
18254_D5,Not found
18256_F5,Not found
18256_F8,Not found
18256_F9,Not found
18257_G5,Not found
18259_B8,Not found
18260_C11,Not found
18260_C2,Not found
18260_C6,Not found
18261_D3,Not found
18261_D5,Not found
18263_A12,Not found
18263_A4,Not found
18264_B3,Not found
18264_B4,Not found
18264_B8,Not found
18265_C10,Not found
18265_C4,Not found
18266_D1,Not found
18266_D6,Not found
18267_E10,Not found
18267_E3,Not found
18267_E4,Not found
18267_E6,Not found
18268_F1,Not found
18268_F12,Not found
18268_F3,Not found
18268_F7,Not found
18268_F8,Not found
18311_A4,Not found
18312_B4,Not found
18312_B5,Not found
18313_C1,Not found
18313_C10,Not found
18313_C11,Not found
18313_C12,Not found
18313_C5,Not found
18313_C7,Not found
18313_C8,Not found
18314_D2,Not found
18314_D3,Not found
18314_D4,Not found
18314_D7,Not found
18315_E12,Not found
18315_E2,Not found
18315_E4,Not found
18315_E6,Not found
18316_F1,Not found
18316_F10,Not found
18316_F11,Not found
18316_F5,Not found
18317_G3,Not found
18317_G4,Not found
18317_G7,Not found
18317_G8,Not found
18317_G9,Not found
#+end_example

Those were the libraries that weren't submitted...

However, Idan would prefer the whole thing to be replaced with the
newer pooled model.

Working on preparing submission, started altering
prepare_submission_20180430.py to support reading different files.

#+NAME run prepare submission
#+BEGIN_SRC bash
PYTHONPATH=/woldlab/loxcyc/home/diane/proj/htsworkflow:/woldlab/loxcyc/home/diane/src/pandasodf python3 \
  prepare_submission_20180430.py \
     --name submission-20180619 \
     --sheet cells_in_paper \
     --header 0 \
     compare_lists_submitted_cells_vs_cells_in_paper_May4_2018.ods
#+END_SRC

Unfortunately the mirror of htsworkflow is broken on felcat.


** <2018-06-20 Wed> Notes

Fixed. The felcat_demo settings file refered to the old varaiable name.

The spreadsheet
compare_lists_submitted_cells_vs_cells_in_paper_May4_2018.ods contains
all the cells...

I had been assuming it was updates to the first set, and not the new
set. but it pretty clearly contains references to the libraries in the
second set.


| ENCSR255XZG | c1_e13.5_mouse_limb_run8  | resubmit |
| ENCSR530WGW | c1_e13.5_mouse_limb_run4  | resubmit |
| ENCSR985VMP | c1_e12.5_mouse_limb_run7  | resubmit |
| ENCSR463HWT | c1_e11.5_mouse_limb_run6  | resubmit |
| ENCSR619BUX | c1_e11.0_mouse_limb_run5  | resubmit |
| ENCSR991VTV | c1_e10.5_mouse_limb_run3  | resubmit |
| ENCSR159DIX | c1_e10.5_mouse_limb_run1  | resubmit |
| ENCSR242DQC | c1_e10.5_mouse_limb_run2  | resubmit |


|             | C1_mouse_e11.5_forelimb_run10 |   |
| ENCSR938RJZ | c1_e14.0_mouse_limb_run13     |   |
| ENCSR546KIB | c1_e14.5_mouse_limb_run17     |   |
| ENCSR430OIC | c1_e15.5_mouse_limb_run14     |   |

** <2018-06-21 Thu>

Tracking by hand is a pain, lets write a program!

wrote [[file:check_paper_cells.py]] which generated tranche.cvs

#+BEGIN_SRC bash
python3 check_paper_cells.py --header 0 compare_lists_submitted_cells_vs_cells_in_paper_May4_2018.ods
#+END_SRC

All of the missing cells are in tranche 1. (AKA from before I
submitted as a pool).

I unfortunately also discovered that some how two libraries were in
two experiments.

| 19912_G1 | 2 | c1_e10.5_mouse_limb_run15, c1_e11.5_mouse_limb_run10 | ENCSR134JVO, ENCSR220RKA |
| 19915_B1 | 2 | c1_e14.5_mouse_limb_run17, c1_e11.0_mouse_limb_run16 | ENCSR546KIB, ENCSR160DGP |

Turns out the DCC supports substring matching and 19912_G1 also
matches 19912_G10.

I updated check_paper_cells to search by the library ID returned when
retrieving the library object given the alias.

(Also I moved the lookup into a function so I could call it as a one
off)

With that change the conflict appears to have been resolved.

Reran and looked at tranche again. Yes it has been fixed.

(I really should commit stuff)


** <2018-06-22 Fri>

How, did I submit the first 433?

Its not in encode3-curation, it's not in encode4-curation. Google
drive has [[https://docs.google.com/spreadsheets/d/1cV9qOFij8K1pjzPFxm-bFR2KlI4I191pVM0nuzxT2jQ/edit#gid=310682961][dcc_experiment_files]] which only has a single sheet.

C1_mouse_limb_combined has tranche.csv (a map of stuff at the DCC),
publication_files.tsv, and README.org

It's not in ~/woldlab/ENCODE either. (Zgrep does look in .ods files)

ENCSR255XZG

Found it! [[file:///home/diane/woldlab/ENCODE/C1-encode3-limb-2017.ods][c1-encode3-limb-2017.ods]] Thanks tracker search!

** <2018-06-25 Mon> New metadata file

[[file:~/woldlab/ENCODE/C1-encode3-limb-tranche1-resubmit.ods][file:~/woldlab/ENCODE/C1-encode3-limb-tranche1-resubmit.ods]] updated to
look like the tranche 2 2018-04 submission.

* Request <2018-06-04 Mon> From Brian make a new trackhub

The source xlsx is
[[BED_files_tracks_request_single_cell_black_yellow_June3_2018.xlsx]]

I hacked generate_trackhub some to hard code fewer things

#+NAME generate 20180604 signal trackhub
#+BEGIN_SRC bash
generate_trackhub.py --hub "C1_s20180604" \
                     --short-name "C1_201806_sig"\
                     --long-name "C1 2018 june 4 signal"\
                     --signal BED_files_tracks_request_single_cell_black_yellow_June3_2018.xlsx
#+END_SRC

#+NAME generate 20180604 read trackhub
#+BEGIN_SRC bash
PYTHONPATH=~/proj/trackhub python3 generate_trackhub.py \
    --hub "C1_r20180604" \
    --short-name "C1_201806_read" \
    --long-name "C1 2018 june 4 reads" \
    --reads BED_files_tracks_request_single_cell_black_yellow_June3_2018.xlsx
#+END_SRC
* <2018-07-09 Mon> Work on producing bigwigs for a subset of cluster cells


** Table of cluster memberships
[[file:C1_peng_20180710_cluster_memberships.txt::{'x18317_G10_mm10'}%20{'DarkRed'%20}][C1 cluster memberships peng 20180710]]
What a terrible format

** Quick test case

python3 make_cluster_bigwigs.py -o merged.bw ~sau/flowcells/HFNYNBCX2/20049_F3/20049_F3-mm10-M4-male_genome.bam ~sau/flowcells/HFNYNBCX2/20048_E12/20048_E12-mm10-M4-male_genome.bam

** My program didn't normalize the bigwigs, and apparently they are normalized somehow.

Around chr12:55,862,947-55,864,116
bigwigs were autoscale max was around 17  and 29
while I was seeing more like 85 for my bigwig.
Also my bigwig didn't go to zero, it just continued the previous value
all the way to the next block

I did find utilities in the ucsc tree, that with a lot of file
maniuplation can make a plausible bigwig file

bigWigMerge <list of bigwigs> out.bg
grep chr out.bg > filtered.bg  # to remove spikes
bedSort filtered.bg sorted.bg  # to sort it the way they want
bedGraphToBigWig sorted.bg chrom.sizes result.bw.

Generate the cluster bigwig files
#+BEGIN_SRC bash
python3 find_bigwigs.py > merge_bw_all.sh
nohup bash ./merge_bw_all.sh &
#+END_SRC

* <2018-07-18 Wed> paper Violin plot

For the paper they wanted a violin plot broken out by cell type.

Peng provided a gene number vs cell type file
[[file:violin-paper/peng-example-violin.txt::gene_index,cell_type][Cell type membership]]

I generatated a bunch of plots using a jupyter notebook.

I foolishly had been editing the same cell for a while while trying to
adjust colors, but that got me in trouble with barbara

It worked better to move each new attempt into its own cell
[[file:violin-paper/peng-violin.ipynb][violin-paper/peng-violin.ipynb]]

* <2018-08-06 Mon> recompute C1 coverage statistics

Brian wants me to recompute the median coverage plots with what we've
learned about coverage.

The original versions were with "all genes" and we want single gene
models.

** TODO compute coverage for the 920 C1 cells in the manuscript [2/3]

We needed updated versions of this:

[[file:/ssh:pongo:/woldlab/loxcyc/home/diane/proj/C1_mouse_limb_combined/cells_in_manuscript.median-normalized.coverage.png][C1_mouse_limb_combined/cells_in_manuscript.median-normalized.coverage.png]]

*** DONE using just single gene models.
    CLOSED: [2018-08-08 Wed 11:13]

    - State "DONE"       from "TODO"       [2018-08-08 Wed 11:13]

Runs on wold-clst-4 took several hours last night? why?

The file sizes of the bigwigs were pretty similar. It did turn out
that wold-clst-4 ended up running out of swap.

It looks like the bigwig file were of similiar sizes.

I learned GNU time supports more program statics, its not installed by
default.

It looks like the coverage program uses about 3G of memory.

*** DONE using only protein coding, single model genes
    CLOSED: [2018-08-09 Thu 10:29]

    - State "DONE"       from "TODO"       [2018-08-09 Thu 10:29]
*** TODO using Georgi's refseq annotation set.
Georgis reference is also mm9!
/woldlab/castor/home/georgi/genomes/mm9/Mus_musculus.NCBIM37.67.filtered.gtf

Downloaded [[ftp://ftp.ncbi.nlm.nih.gov/refseq/M_musculus/alignments/GCF_000001635.24_knownrefseq_alignments.gff3][GCF_000001635.24_knownrefseq_alignments.gff3]]
Made working dir [[file:single-cell-single-model-min-1000-refseq]]

Adjusted [[file:single-cell-single-model-min-1000-refseq/recompute-coverage.condor]]
with paths

refSeq using different attributes than gencode. Because of course.


** TODO compute coverage for the 920 C1 cells broken out by their peng cell type cluster membership

Separate them out by color clusters, and then do coverage plots for
each of the clusters.

Delayed because the median summary code isn't per
experiment... woldrnaseq/plot_coverage needs help.

** TODO Compute fixed coverage for the limb bulk libraries
Then corrected coverage plot for all the limb bulks (12 libraries),

| directories | brians list      |
|       15019 | 15019 e11.5 limb |
|       15020 | 15020 e11.5 limb |
|       15084 | 15084 e14.5 limb |
|       15085 | 15085 e14.5 limb |
|       16111 | 16110 e13.5 limb |
|       16112 | 16111 e13.5 limb |
|       16134 | 16134 e15.5 limb |
|       16135 | 16135 e15.5 limb |
|       16930 | 16930 e12.5 limb |
|       16931 | 16931 e12.5 limb |
|       17298 | 17298 e10.5 limb |
|       17299 | 17299 e10.5 limb |

*** <2018-08-24 Fri> built plots

first run qc-coverage.condor arguments
#+BEGIN_EXAMPLE
$(GENE_COVERAGE) --gtf $(GTF) $(IN) --output $(OUT) --print-list
#+END_EXAMPLE

Second run
#+BEGIN_EXAMPLE
$(GENE_COVERAGE) --gtf $(GTF) $(IN) --output $(OUT) --gene-normalization max
#+END_EXAMPLE

This means that the .geneList files are raw un-normalized values.
The .coverage files are max normalized.

In http://woldlab.caltech.edu/~diane/C1_mouse_limb_combined/bulk/
For each experiment
ENCSR216NEG-e14.5-limb.median.coverage.png
ENCSR347SQR-e13.5-limb.median.coverage.png
ENCSR541XZK-e11.5-limb.median.coverage.png
ENCSR750YSX-e12.5-limb.median.coverage.png
ENCSR830IQV-e15.5-limb.median.coverage.png
ENCSR968QHO-e10.5-limb.median.coverage.png

for all the experiments
ENCSR968QHO-e10.5-limb.median-normalized.coverage.bare.png
ENCSR968QHO-e10.5-limb.median-normalized.coverage.png

Detail coverage plots
http://woldlab.caltech.edu/~diane/C1_mouse_limb_combined/bulk/gene_coverage_detail/


** TODO Compute fixed coverage for all of the C1 bulk libraries.

There's about 130 total C1 bulk libraries.

Include spearman calculations: so basically re run the QC steps.

** TODO Compute spike detection probabilities for the 920 cells
** TODO Compute spike detection probabilities for the 920 cells by peng cell type cluster membership
** TODO Coverage for top 1000 protein coding genes

Can we try a short experiment?  Pull up any single cell library that you have already made a transcript coverage plot for.
Then, take the FPKM values for the top 1000 most highly expressed genes in that library (you can use protein coding), regardless of
model complexity.
* <2018-09-27 Thu> Jason told me that there were mixed length reads in 3 fastqs

| ENCFF690WSL | C1_mouse_e13.5_limb_mesenchyme_mm10_run4.fastq.gz |
| ENCFF610BCY | C1_mouse_e11.5_limb_mesenchyme_mm10_run6.fastq.gz |
| ENCFF035IIS | C1_mouse_e11.0_limb_mesenchyme_mm10_run5.fastq.gz |

First I committed all of the previous work in the
C1_mouse_limb_combined directory.

Then I modified prepare_submission_20180430 to pass the per fastq url
metadata into the code to generate the fastq merging condor file.

(Amusingly that made the program simpler and cleaner)

#+BEGIN_SRC bash
python3 prepare_submission_20180430.py  \
  --first-tranche \
  --name submission-20180926 \
  --sheet cells_in_paper \
  --header 0 \
  compare_lists_submitted_cells_vs_cells_in_paper_May4_2018.ods

#+END_SRC

It looks like the sheet to describe what files to upload is in the ods
file, so I can just modify that after they're finished building.

** <2018-09-28 Fri> Double checking fastqs

- Do we have the same total length?

#+BEGIN_SRC bash
for f in \
  C1_mouse_e13.5_limb_mesenchyme_mm10_run4_101.fastq.gz \
  C1_mouse_e13.5_limb_mesenchyme_mm10_run4_51.fastq.gz \
  C1_mouse_e13.5_limb_mesenchyme_mm10_run4.fastq.gz \
  C1_mouse_e11.0_limb_mesenchyme_mm10_run5_101.fastq.gz \
  C1_mouse_e11.0_limb_mesenchyme_mm10_run5_51.fastq.gz \
  C1_mouse_e11.0_limb_mesenchyme_mm10_run5.fastq.gz \
  C1_mouse_e11.5_limb_mesenchyme_mm10_run6_101.fastq.gz \
  C1_mouse_e11.5_limb_mesenchyme_mm10_run6_51.fastq.gz \
  C1_mouse_e11.5_limb_mesenchyme_mm10_run6.fastq.gz; do \
  echo $f $(zcat  $f | wc -l); \
done
#+END_SRC

| C1_mouse_e13.5_limb_mesenchyme_mm10_run4_101.fastq.gz |  89366592 |
| C1_mouse_e13.5_limb_mesenchyme_mm10_run4_51.fastq.gz  | 304263328 |
|                                                       | 393629920 |
| C1_mouse_e13.5_limb_mesenchyme_mm10_run4.fastq.gz     | 393629920 |
| C1_mouse_e11.0_limb_mesenchyme_mm10_run5_101.fastq.gz |  96328608 |
| C1_mouse_e11.0_limb_mesenchyme_mm10_run5_51.fastq.gz  | 446122700 |
|                                                       | 542451308 |
| C1_mouse_e11.0_limb_mesenchyme_mm10_run5.fastq.gz     | 542451308 |
| C1_mouse_e11.5_limb_mesenchyme_mm10_run6_101.fastq.gz | 348093872 |
| C1_mouse_e11.5_limb_mesenchyme_mm10_run6_51.fastq.gz  | 537210244 |
|                                                       | 885304116 |
| C1_mouse_e11.5_limb_mesenchyme_mm10_run6.fastq.gz     | 885304116 |


** Did we submit what we're supposed to?

| C1_e10.5_mouse_limb_run1_June6_2016            |  87 |
| C1_e10.5_mouse_limb_run2_June20_2016           |  56 |
| C1_e10.5_mouse_limb_run3_Dec5_2016             |  32 |
| C1_mouse_e11.0_limb_mesenchyme_mm10_run5       |  58 |
| C1_mouse_e11.5_limb_mesenchyme_mm10_run6       |  80 |
| C1_mouse_e12.5_limb_mesenchyme_mm10_run7       |  76 |
| C1_mouse_e13.5_limb_mesenchyme_mm10_run4       |  71 |
| C1_mouse_e13.5_limb_mesenchyme_mm10_run8       |  64 |
| C1_mouse_e10.5_forelimb_run15_January13AM_2018 |  18 |
| C1_mouse_e11.0_forelimb_run16_January13PM_2018 |  14 |
| C1_mouse_e11.5_forelimb_run10_December11_2017  |  77 |
| C1_mouse_e12.0_forelimb_run11_December12_2017  |  77 |
| C1_mouse_e13.0_forelimb_run12_December13_2017  |  72 |
| C1_mouse_e14.0_forelimb_run13_December14_2017  |  53 |
| C1_mouse_e14.5_forelimb_run17_January16_2018   |  27 |
| C1_mouse_e15.5_forelimb_run14_December15_2017  |  58 |
|                                                | 920 |
#+TBLFM: @17$2=vsum(@1$2..@16$2)

Paper sheet says 920 sum of aliases in tsvs sumbs to 920.
whew

** Barbara wants all the fastqs to be the same length. 50.

Built a set of submission metadata with "everything" included

#+BEGIN_SRC python :results output
import pandas
from pprint import pprint
df = pandas.read_csv('submission-20180928-flowcell-details.tsv', sep='\t')
pprint(set(df[df['read_length'] == 101]['experiment']))

#+END_SRC

#+RESULTS:
: {'C1_e10.5_mouse_limb_run3_Dec5_2016',
:  'C1_mouse_e13.5_limb_mesenchyme_mm10_run4',
:  'C1_mouse_e11.0_limb_mesenchyme_mm10_run5',
:  'C1_mouse_e11.5_limb_mesenchyme_mm10_run6',
:  'C1_mouse_e14.0_forelimb_run13_December14_2017'}

Those are the ones with 100 bp

** fastq replacements

| old         | submitted_file_name                                    | action  | replacment  |
| ENCFF567DVD | C1_e10.5_mouse_limb_run3_Dec5_2016.fastq.gz            | replace | ENCFF949JJP |
| ENCFF690WSL | C1_mouse_e13.5_limb_mesenchyme_mm10_run4.fastq.gz      | replace | ENCFF738JJC |
| ENCFF035IIS | C1_mouse_e11.0_limb_mesenchyme_mm10_run5.fastq.gz      | replace | ENCFF710CRO |
| ENCFF610BCY | C1_mouse_e11.5_limb_mesenchyme_mm10_run6.fastq.gz      | replace | ENCFF653CRU |
| ENCFF615WYP | C1_mouse_e13.5_limb_mesenchyme_mm10_run4_101.fastq.gz  | delete  |             |
| ENCFF634YJN | C1_mouse_e13.5_limb_mesenchyme_mm10_run4_51.fastq.gz   | delete  |             |
| ENCFF821PYB | C1_mouse_e11.0_limb_mesenchyme_mm10_run5_101.fastq.gz  | delete  |             |
| ENCFF310FGP | C1_mouse_e11.0_limb_mesenchyme_mm10_run5_51.fastq.gz   | delete  |             |
| ENCFF164AIS | C1_mouse_e11.5_limb_mesenchyme_mm10_run6_101.fastq.gz  | delete  |             |
| ENCFF121IGH | C1_mouse_e11.5_limb_mesenchyme_mm10_run6_51.fastq.gz   | delete  |             |
| ENCFF924SYO | C1_mouse_e14.0_forelimb_run13_December14_2017.fastq.gz | replace | ENCFF749NGB |

** <2018-10-01 Mon> Uploaded the replacement files today

I had to tack the run13 information from
submission-201804-flowcell-details.tsv to the end of
submission-20180926-flowcell-details+run13.tsv to get the
flowcel_details field to be filled out correctly.

